# Cronicorn Demo Video Rubric

**Purpose:** Evaluation framework for creating and assessing a conversion-focused product demo video that introduces new users to Cronicorn for the first time.

**Target Length:** 90-120 seconds (optimal for first-time viewers based on 2025 industry data)

**Primary Goal:** Convert visitors into trial users by clearly demonstrating value and increasing product understanding

---

## ðŸ“Š Scoring System

Each criterion is scored 1-5:
- **5** = Exceptional - Best-in-class execution
- **4** = Strong - Meets all requirements with polish
- **3** = Adequate - Functional but room for improvement
- **2** = Weak - Missing key elements or poorly executed
- **1** = Poor - Needs major revision

**Target Score:** 80/100 (4.0 average) for production-ready demo

---

## ðŸŽ¯ Part 1: Hook & Problem Statement (0-15 seconds)

### Criterion 1.1: Immediate Value Hook (Weight: 10 points)

**What to Evaluate:**
- Does the first 3 seconds grab attention with a relatable problem?
- Is the hook specific to the target audience (DevOps/SRE engineers)?
- Does it create curiosity without giving everything away?

**Scoring Guide:**
- **5**: Opens with visceral, relatable pain point that makes target audience immediately think "that's me"
  - Example: "Getting paged at 3 AM for database blips that fix themselves in 30 seconds"
- **4**: Clear problem statement that resonates with target audience
- **3**: Generic problem statement that could apply to any scheduler
- **2**: Weak hook that takes too long to get to the point
- **1**: No clear hook or starts with features instead of problems

**Cronicorn-Specific Success Indicators:**
- âœ… References alert fatigue, firefighting, or rigid scheduling
- âœ… Uses language from brand voice (human, relatable, technical)
- âœ… Immediately signals "this is for engineers" not generic marketing

**Examples:**
- âœ… **Strong**: "Your health checks run every 5 minutesâ€”whether your API is on fire or humming along perfectly."
- âŒ **Weak**: "Cronicorn is an advanced AI-powered scheduling platform..."

---

### Criterion 1.2: Problem Context (Weight: 5 points)

**What to Evaluate:**
- Does it quickly establish why static scheduling is limiting?
- Is the pain quantified or made tangible?
- Does it avoid over-explaining?

**Scoring Guide:**
- **5**: Provides just enough context to establish credibility without dwelling on problems
- **4**: Clear problem context with 1-2 specific examples
- **3**: Generic problem description without specificity
- **2**: Either too vague or too lengthy
- **1**: Skips problem context entirely or confuses the viewer

**Cronicorn-Specific Success Indicators:**
- âœ… Shows understanding of real DevOps workflows
- âœ… References concrete scenarios (traffic surges, incidents, rate limits)
- âœ… Ties to measurable outcomes (80% alert reduction, 10x faster resolution)

---

## ðŸ’¡ Part 2: Solution Introduction (15-30 seconds)

### Criterion 2.1: Value Proposition Clarity (Weight: 10 points)

**What to Evaluate:**
- Is the solution immediately clear and different from competitors?
- Does it communicate the "adaptive" benefit without jargon?
- Can a viewer explain what Cronicorn does after this section?

**Scoring Guide:**
- **5**: Crystal clear one-sentence value prop that differentiates from traditional schedulers
  - Example: "Cronicorn is an AI job scheduler that adapts to your system's reality"
- **4**: Clear value prop with minor room for simplification
- **3**: Understandable but requires careful listening
- **2**: Confusing or feature-focused instead of benefit-focused
- **1**: Unclear what the product actually does

**Cronicorn-Specific Success Indicators:**
- âœ… Uses one of the approved taglines/value props from marketing docs
- âœ… Emphasizes "adaptive" or "intelligent" scheduling
- âœ… Immediately differentiates from "traditional cron jobs"
- âœ… Avoids "AI magic" language (stays transparent)

**Reference Materials:**
- Primary tagline: "Intelligent job scheduling that adapts to your reality"
- Elevator pitch: "AI-powered job scheduler that automatically adapts to your system's reality"

---

### Criterion 2.2: Differentiation (Weight: 5 points)

**What to Evaluate:**
- Does it explain how Cronicorn is different from alternatives?
- Is the differentiation compelling and believable?
- Does it avoid negative competitor comparisons?

**Scoring Guide:**
- **5**: Clearly positions against "static schedulers" without naming competitors, emphasizes transparency
- **4**: Clear differentiation with good contrast
- **3**: Some differentiation but not compelling
- **2**: Weak or confusing positioning
- **1**: No differentiation or negative competitor bashing

**Cronicorn-Specific Success Indicators:**
- âœ… Contrasts static vs. adaptive scheduling
- âœ… Mentions "every decision explained" (transparency differentiator)
- âœ… Highlights min/max constraints (user control)
- âŒ Avoid: "Better than X" or competitor name-dropping

---

## ðŸŽ¬ Part 3: Product Demonstration (30-90 seconds)

### Criterion 3.1: Use Case Storytelling (Weight: 15 points)

**What to Evaluate:**
- Does the demo show a real, relatable scenario instead of generic features?
- Is the user journey clear and logical?
- Does it show before/after or problem/solution in action?

**Scoring Guide:**
- **5**: Demonstrates complete scenario that target audience recognizes immediately with clear before/after
  - Example: Flash sale scenario showing 5min â†’ 30sec adaptation â†’ back to 5min
- **4**: Good scenario with clear progression but minor gaps in storytelling
- **3**: Shows features in context but lacks narrative flow
- **2**: Feature tour without real-world context
- **1**: Random feature showcasing without coherent story

**Cronicorn-Specific Success Indicators:**
- âœ… Uses one of the documented use cases (DevOps monitoring, e-commerce, data pipelines)
- âœ… Shows the timeline visualization (the GIF in README demonstrates this well)
- âœ… Demonstrates complete workflow: detect â†’ adapt â†’ recover â†’ stabilize
- âœ… Makes AI reasoning visible ("Traffic surge detectedâ€”tightening to 30s")

**Recommended Use Case for Demo:**
Primary: DevOps monitoring with incident response (most universal)
Backup: E-commerce flash sale (more visual, shows dramatic adaptation)

---

### Criterion 3.2: Visual Clarity (Weight: 10 points)

**What to Evaluate:**
- Are screen recordings clear and easy to follow?
- Is text readable at typical viewing sizes?
- Are important UI elements highlighted or called out?
- Is pacing appropriate (not too fast or slow)?

**Scoring Guide:**
- **5**: Professional screen recordings with strategic highlights, annotations, smooth transitions
- **4**: Clear visuals with good pacing and mostly readable text
- **3**: Adequate clarity but some elements hard to see or too fast
- **2**: Poor visual quality, hard to follow, or confusing jumps
- **1**: Unclear screen recordings or disorienting presentation

**Cronicorn-Specific Success Indicators:**
- âœ… Shows dashboard timeline visualization clearly
- âœ… Highlights AI reasoning/explanations in UI
- âœ… Demonstrates both API and web UI (versatility)
- âœ… Uses zoom/highlight for important details
- âœ… Smooth transitions between screens (professional feel)

**Technical Specifications:**
- Minimum 1080p resolution
- Screen recordings at consistent zoom level
- Clear mouse movements (not erratic)
- Readable font sizes (16px+ for body text when shown)

---

### Criterion 3.3: Feature-to-Benefit Translation (Weight: 10 points)

**What to Evaluate:**
- For each feature shown, is the benefit immediately clear?
- Does narration focus on "what this means for you" not just "what this does"?
- Are technical capabilities translated to outcomes?

**Scoring Guide:**
- **5**: Every feature shown is immediately connected to concrete benefit with outcome metrics
  - Example: "AI tightens to 30 seconds [FEATURE] â†’ catch issues 10x faster [BENEFIT]"
- **4**: Most features clearly tied to benefits
- **3**: Some benefit translation but occasional feature-only descriptions
- **2**: Mostly feature descriptions without clear benefits
- **1**: Pure feature tour with no benefit context

**Cronicorn-Specific Success Indicators:**
- âœ… "Adaptive intervals" â†’ "Reduce alert fatigue by 80%"
- âœ… "AI reasoning visible" â†’ "Always know why schedules changed"
- âœ… "Min/max constraints" â†’ "You stay in control"
- âœ… "Auto-recovery workflows" â†’ "Fix issues before paging oncall"

**Key Benefit Metrics to Include:**
- 80% reduction in alert fatigue
- 10x faster issue resolution
- Zero schedule maintenance

---

### Criterion 3.4: Transparency & Trust Building (Weight: 10 points)

**What to Evaluate:**
- Does the demo show how AI makes decisions (not black box)?
- Are limitations or constraints mentioned?
- Does it build credibility through honesty?

**Scoring Guide:**
- **5**: Explicitly shows AI reasoning, mentions constraints, demonstrates user control mechanisms
- **4**: Good transparency with most trust elements present
- **3**: Some transparency but could be more explicit
- **2**: Vague about how AI works or overpromises
- **1**: Black box presentation or makes unrealistic claims

**Cronicorn-Specific Success Indicators:**
- âœ… Shows exact AI reasoning message: "Traffic surge detectedâ€”tightening monitoring to 30 seconds"
- âœ… Demonstrates min/max constraint enforcement
- âœ… Mentions that AI is optional: "Works perfectly without AI"
- âœ… Shows graceful degradation: "Baseline schedule continues if AI unavailable"
- âœ… Avoids: "AI magic," "just works," or hand-wavy explanations

**Brand Voice Alignment:**
This criterion is critical for Cronicorn's "Transparent" brand pillarâ€”we show our work, no black boxes.

---

## ðŸŽ¤ Part 4: Production Quality (Throughout)

### Criterion 4.1: Audio Quality (Weight: 5 points)

**What to Evaluate:**
- Is voiceover clear, professional, and easy to understand?
- Is audio level consistent throughout?
- Is pacing appropriate (not too fast or monotone)?

**Scoring Guide:**
- **5**: Professional voiceover with perfect clarity, engaging tone, appropriate pacing
- **4**: Clear audio with minor imperfections
- **3**: Understandable but amateur quality or pacing issues
- **2**: Distracting audio issues (echo, volume jumps, unclear speech)
- **1**: Poor audio quality that hinders comprehension

**Cronicorn-Specific Success Indicators:**
- âœ… Tone matches brand voice: "Senior engineer explaining to colleague"
- âœ… Conversational but confident (not corporate or overly casual)
- âœ… Technical terms pronounced correctly
- âŒ Avoid: Over-enthusiastic sales voice or monotone reading

---

### Criterion 4.2: Visual Polish (Weight: 5 points)

**What to Evaluate:**
- Are transitions smooth and professional?
- Is branding consistent (logo, colors, fonts)?
- Are animations purposeful (not gratuitous)?
- Does it look modern and trustworthy?

**Scoring Guide:**
- **5**: Polished production with branded elements, smooth transitions, purposeful motion
- **4**: Professional look with minor rough edges
- **3**: Adequate quality but feels amateur in places
- **2**: Distracting visual issues or inconsistent branding
- **1**: Poor production quality that undermines credibility

**Cronicorn-Specific Success Indicators:**
- âœ… Uses brand blue (#3B82F6) consistently
- âœ… Logo appears appropriately (start/end, not distracting)
- âœ… Matches modern developer tool aesthetic (Linear, Vercel, Stripe style)
- âœ… Subtle animations that enhance understanding
- âŒ Avoid: Over-designed motion graphics or stock footage

---

## ðŸ“ž Part 5: Call-to-Action (90-120 seconds)

### Criterion 5.1: CTA Clarity (Weight: 5 points)

**What to Evaluate:**
- Is the next step immediately obvious?
- Is there a single, clear primary action?
- Does it reduce friction (no credit card, free trial, etc.)?

**Scoring Guide:**
- **5**: Single, crystal clear CTA with friction reducers and compelling reason to act now
  - Example: "Start scheduling smarterâ€”free trial, no credit card required"
- **4**: Clear CTA with minor room for improvement
- **3**: CTA present but not compelling or slightly confusing
- **2**: Weak CTA or multiple competing actions
- **1**: No clear CTA or confusing next steps

**Cronicorn-Specific Success Indicators:**
- âœ… Primary CTA: "Get Early Access" or "Start Free Trial"
- âœ… Includes friction reducers: "No credit card required," "14-day free trial"
- âœ… Shows URL clearly: cronicorn.com or docs.cronicorn.com
- âœ… Optional secondary CTA: "View Documentation" or "See API Playground"

**Approved CTAs from Marketing Docs:**
- "Get Early Access"
- "Start Free Trial"
- "See It In Action"

---

### Criterion 5.2: Momentum & Urgency (Weight: 5 points)

**What to Evaluate:**
- Does the ending create desire to try the product now?
- Is there a reason not to wait?
- Does it end on a high note?

**Scoring Guide:**
- **5**: Creates genuine urgency through value reinforcement and clear benefit of acting now
- **4**: Good momentum with compelling ending
- **3**: Adequate close but lacks excitement
- **2**: Weak ending that loses momentum
- **1**: Abrupt ending or actively discourages immediate action

**Cronicorn-Specific Success Indicators:**
- âœ… Reinforces key benefit: "Stop getting paged at 3 AM"
- âœ… References ease: "Set up your first job in 5 minutes"
- âœ… Ends with confidence, not pleading
- âœ… Leaves viewer thinking "I should try this"

---

## ðŸ“ Part 6: Overall Effectiveness

### Criterion 6.1: Length & Pacing (Weight: 5 points)

**What to Evaluate:**
- Is total length within 90-120 second target?
- Does pacing feel appropriate throughout?
- Are any sections too rushed or too slow?

**Scoring Guide:**
- **5**: Perfect 90-120 second length with well-balanced pacing throughout
- **4**: Good length and pacing with minor sections that could be tightened
- **3**: Acceptable length but uneven pacing
- **2**: Too long (>150 sec) or too short (<60 sec), or very uneven pacing
- **1**: Length severely impacts effectiveness or pacing is confusing

**Research-Based Rationale:**
- 60-180 seconds is optimal for SaaS demos viewed independently
- 90-120 seconds hits sweet spot for first-time viewer engagement
- Videos under 3 minutes maintain highest completion rates

---

### Criterion 6.2: Target Audience Alignment (Weight: 5 points)

**What to Evaluate:**
- Would the target audience (DevOps/SRE engineers) find this compelling?
- Does it respect their technical competence?
- Does language and tone match their expectations?

**Scoring Guide:**
- **5**: Perfectly calibrated for technical audienceâ€”respects intelligence while being accessible
- **4**: Good audience fit with minor mismatches
- **3**: Somewhat generic or occasionally talks down/up to audience
- **2**: Poor audience targeting or condescending tone
- **1**: Wrong audience or actively alienating

**Cronicorn-Specific Success Indicators:**
- âœ… Uses technical terms correctly: "endpoints," "webhooks," "cron expressions," "adaptive intervals"
- âœ… References real workflows: "oncall," "paging," "incident response," "monitoring"
- âœ… Respects competence: Doesn't over-explain basics
- âœ… Shows technical depth: API examples, constraint configuration
- âŒ Avoid: "Magic," "simple," "easy" (can sound patronizing)

**Secondary Audiences:**
- Engineering Managers: Focus on team efficiency, cost reduction
- CTOs: Focus on scalability, competitive advantage

---

## ðŸ“‹ Scoring Summary Template

| Category | Criterion | Weight | Score (1-5) | Weighted |
|----------|-----------|--------|-------------|----------|
| **Hook & Problem** | 1.1 Immediate Value Hook | 10 | ___ | ___ |
| | 1.2 Problem Context | 5 | ___ | ___ |
| **Solution Intro** | 2.1 Value Prop Clarity | 10 | ___ | ___ |
| | 2.2 Differentiation | 5 | ___ | ___ |
| **Demonstration** | 3.1 Use Case Storytelling | 15 | ___ | ___ |
| | 3.2 Visual Clarity | 10 | ___ | ___ |
| | 3.3 Feature-to-Benefit | 10 | ___ | ___ |
| | 3.4 Transparency & Trust | 10 | ___ | ___ |
| **Production** | 4.1 Audio Quality | 5 | ___ | ___ |
| | 4.2 Visual Polish | 5 | ___ | ___ |
| **Call-to-Action** | 5.1 CTA Clarity | 5 | ___ | ___ |
| | 5.2 Momentum & Urgency | 5 | ___ | ___ |
| **Overall** | 6.1 Length & Pacing | 5 | ___ | ___ |
| | 6.2 Audience Alignment | 5 | ___ | ___ |
| **TOTAL** | | **100** | | **___/100** |

**Interpretation:**
- **90-100**: Exceptional - Production-ready, best-in-class demo
- **80-89**: Strong - Ready for production with minor polish
- **70-79**: Good - Solid foundation, needs targeted improvements
- **60-69**: Adequate - Functional but requires significant refinement
- **Below 60**: Needs Major Revision - Address fundamental issues before production

---

## ðŸŽ¬ Recommended Demo Structure (90-120 seconds)

### Section 1: Hook & Problem (0-15 seconds)
**Visual**: Quick montage or single powerful shot
**Narration**: "Getting paged at 3 AM for database blips that fix themselves in 30 seconds? Your health checks run every 5 minutesâ€”whether your API is on fire or humming along perfectly."
**On-screen text**: Optional problem stat

### Section 2: Solution Introduction (15-30 seconds)
**Visual**: Cronicorn dashboard/logo reveal
**Narration**: "Cronicorn is an AI-powered job scheduler that adapts to your system's reality. Instead of rigid schedules, it tightens monitoring during incidents and relaxes during recovery."
**On-screen text**: Value prop tagline

### Section 3: Use Case Demo (30-85 seconds)
**Visual**: Screen recording of complete scenario
**Narration**: Walk through specific use case showing:
- Baseline schedule running
- Issue detection / traffic surge
- AI reasoning visible: "Traffic surge detectedâ€”tightening to 30 seconds"
- Timeline showing adaptation
- Recovery and return to baseline
- Result: "80% fewer false alerts, 10x faster detection"

**On-screen highlights**: Call out AI reasoning, constraints, transparency

### Section 4: Key Differentiators (85-105 seconds)
**Visual**: Quick feature highlights
**Narration**: "Every decision explained. Min/max constraints keep you in control. Works perfectly with or without AI."
**On-screen text**: Bullet points of key benefits

### Section 5: Call-to-Action (105-120 seconds)
**Visual**: Dashboard with CTA overlay
**Narration**: "Stop getting paged at 3 AM. Start scheduling smarter."
**On-screen text**: "Get Early Access â€” cronicorn.com â€” No credit card required"

---

## ðŸŽ¯ Success Metrics (Post-Launch)

Once the demo video is deployed, track these metrics:

**Engagement Metrics:**
- View completion rate (Target: >65% for 90-120 sec video)
- Average watch time (Target: >75 seconds)
- Replay rate (Target: >10%)

**Conversion Metrics:**
- Landing page conversion rate with video vs. without (Target: +80% improvement)
- Click-through rate from video to CTA (Target: >15%)
- Trial sign-ups attributed to video (Track via UTM parameters)

**Qualitative Feedback:**
- User interviews: "What did you learn from the demo?"
- Support tickets: Does video reduce confusion questions?
- Social sharing: Are people organically sharing the video?

---

## ðŸ”„ Iteration Framework

After initial launch:

**Week 1-2: Gather Data**
- Watch session recordings of users viewing the demo
- Track drop-off points in video analytics
- Collect qualitative feedback from trial users

**Week 3-4: Analyze & Prioritize**
- Identify sections where viewers drop off
- Note frequently asked questions after viewing
- Prioritize improvements based on impact/effort

**Month 2: Test Variations**
- A/B test different hooks or CTAs
- Try alternate use case scenarios
- Test different video lengths

**Quarterly: Major Updates**
- Refresh demo as product evolves
- Update based on new marketing insights
- Incorporate successful customer stories

---

## ðŸ“š Reference Examples to Study

### Developer Tool Demos (Study These)
- **Vercel**: Clean, fast-paced, developer-focused
- **Linear**: Modern aesthetic, confident tone, problem-solution structure
- **Stripe**: Clear explanations, respects technical audience
- **Notion**: Complete workflow demonstration, approachable
- **Slack**: Casual conversation style, quick onboarding feel

### What Makes Them Successful
- **Problem-first approach**: Start with relatable pain
- **Real scenarios**: Not generic feature tours
- **Technical respect**: Don't oversimplify for engineers
- **Clarity over flash**: Professional but not over-produced
- **Clear value**: Benefit of each feature is obvious

### What to Avoid
- **Feature dumps**: Listing capabilities without context
- **Generic stock footage**: Feels inauthentic for dev tools
- **Corporate voice**: Too formal for developer audience
- **Hype language**: "Revolutionary," "game-changing," etc.
- **No clear CTA**: Leaving viewers unsure what to do next

---

## âœ… Pre-Production Checklist

Before creating the demo video, ensure:

**Content Planning:**
- [ ] Use case selected and validated with target users
- [ ] Script written and scored against rubric
- [ ] Key features to demonstrate identified (max 3-4)
- [ ] Benefits clearly articulated for each feature
- [ ] AI transparency moments scripted
- [ ] CTA and friction reducers finalized

**Production Preparation:**
- [ ] Demo environment set up with clean data
- [ ] Screen recordings planned and storyboarded
- [ ] Brand assets gathered (logo, colors, fonts)
- [ ] Voiceover artist selected (or AI voice tested)
- [ ] Timeline visualization created/ready to show
- [ ] Annotations and highlights planned

**Technical Requirements:**
- [ ] Recording software tested (1080p minimum)
- [ ] Audio equipment/setup tested
- [ ] Video editing software selected
- [ ] Export settings determined (format, resolution, compression)
- [ ] Hosting platform selected (YouTube, Vimeo, self-hosted)

**Brand Alignment:**
- [ ] Script reviewed against brand voice guidelines
- [ ] Messaging approved by marketing
- [ ] Technical accuracy verified by engineering
- [ ] Tone matches "senior engineer to colleague"
- [ ] Transparency requirements met (show AI reasoning)

---

## ðŸŽ“ Training: How to Use This Rubric

### For Video Creators:
1. **Script Phase**: Score your script draft against criteria before production
2. **Production**: Reference checklist to ensure all elements are captured
3. **Post-Production**: Self-evaluate rough cut and iterate before final
4. **Final Review**: Score final video and ensure >80/100 target

### For Reviewers:
1. **Independent Scoring**: Watch video and score each criterion individually
2. **Calibration**: Compare scores with other reviewers and discuss differences
3. **Prioritized Feedback**: Focus on lowest-scoring high-weight criteria first
4. **Actionable Notes**: Provide specific examples for improvements

### For Stakeholders:
1. **Quick Assessment**: Review total score and category breakdowns
2. **Focus Areas**: Identify which sections need most attention
3. **Go/No-Go**: Use 80/100 threshold for production readiness decision
4. **Iteration Planning**: Prioritize improvements based on weighted scores

---

## ðŸ“ž Questions & Revisions

This rubric should evolve based on:
- **User testing**: What actually converts viewers to trial users?
- **A/B testing**: Which approaches perform best?
- **Industry trends**: What are competitors doing successfully?
- **Product evolution**: As Cronicorn matures, demo should too

**Review quarterly** and update based on:
- Video performance metrics
- User feedback from trial sign-ups
- Changes to product positioning
- New features or capabilities

---

**Version:** 1.0
**Created:** 2026-01-06
**Last Updated:** 2026-01-06
**Owner:** Marketing/Growth Team
**Status:** Ready for Use

---

## ðŸ”— Related Documents

- [Marketing Overview](./overview.md) - Overall marketing strategy and positioning
- [Brand Voice Guidelines](./brand-voice.md) - Tone, style, and messaging framework
- [Copy Cheat Sheet](./copy-cheatsheet.md) - Quick reference for messaging
- [Landing Page Blueprints](./page-blueprints.md) - Website content structure
- [SEO Strategy](./seo-strategy.md) - Video SEO and distribution

---

**Key Takeaway**: This rubric emphasizes **problem-first storytelling**, **transparent AI demonstration**, and **technical respect** for the DevOps/SRE audience. A successful Cronicorn demo video should make engineers think "this solves my exact problem" within the first 15 seconds and show them exactly how within the next 75 seconds.
