# Cronicorn Demo Video Rubric

**Purpose:** Evaluation framework for creating and assessing a conversion-focused product demo video that introduces new users to Cronicorn for the first time.

**Target Length:** 90-120 seconds (optimal for first-time viewers based on 2025 industry data)

**Primary Goal:** Convert visitors into trial users by clearly demonstrating value and increasing product understanding

---

## ðŸ“Š Scoring System

Each criterion is scored 1-5:
- **5** = Exceptional - Best-in-class execution
- **4** = Strong - Meets all requirements with polish
- **3** = Adequate - Functional but room for improvement
- **2** = Weak - Missing key elements or poorly executed
- **1** = Poor - Needs major revision

**Target Score:** 80/100 (4.0 average) for production-ready demo

---

## ðŸŽ¯ Part 1: Hook & Problem Statement (0-15 seconds)

### Criterion 1.1: Immediate Value Hook (Weight: 10 points)

**What to Evaluate:**
- Does the first 3 seconds grab attention with a relatable problem?
- Is the hook specific to the target audience (DevOps/SRE engineers)?
- Does it create curiosity without giving everything away?

**Scoring Guide:**
- **5**: Opens with visceral, relatable pain point that makes target audience immediately think "that's me"
  - Example: "Getting paged at 3 AM for database blips that fix themselves in 30 seconds"
- **4**: Clear problem statement that resonates with target audience
- **3**: Generic problem statement that could apply to any scheduler
- **2**: Weak hook that takes too long to get to the point
- **1**: No clear hook or starts with features instead of problems

**Cronicorn-Specific Success Indicators:**
- âœ… References alert fatigue, firefighting, or rigid scheduling
- âœ… Uses language from brand voice (human, relatable, technical)
- âœ… Immediately signals "this is for engineers" not generic marketing

**Examples:**
- âœ… **Strong**: "Your health checks run every 5 minutesâ€”whether your API is on fire or humming along perfectly."
- âŒ **Weak**: "Cronicorn is an advanced AI-powered scheduling platform..."

---

### Criterion 1.2: Problem Context (Weight: 5 points)

**What to Evaluate:**
- Does it quickly establish why static scheduling is limiting?
- Is the pain quantified or made tangible?
- Does it avoid over-explaining?

**Scoring Guide:**
- **5**: Provides just enough context to establish credibility without dwelling on problems
- **4**: Clear problem context with 1-2 specific examples
- **3**: Generic problem description without specificity
- **2**: Either too vague or too lengthy
- **1**: Skips problem context entirely or confuses the viewer

**Cronicorn-Specific Success Indicators:**
- âœ… Shows understanding of real DevOps workflows
- âœ… References concrete scenarios (traffic surges, incidents, rate limits)
- âœ… Ties to measurable outcomes (80% alert reduction, 10x faster resolution)

---

## ðŸ’¡ Part 2: Solution Introduction (15-30 seconds)

### Criterion 2.1: Value Proposition Clarity (Weight: 10 points)

**What to Evaluate:**
- Is the solution immediately clear and different from competitors?
- Does it communicate the "adaptive" benefit without jargon?
- Can a viewer explain what Cronicorn does after this section?

**Scoring Guide:**
- **5**: Crystal clear one-sentence value prop that differentiates from traditional schedulers
  - Example: "Cronicorn is an AI job scheduler that adapts to your system's reality"
- **4**: Clear value prop with minor room for simplification
- **3**: Understandable but requires careful listening
- **2**: Confusing or feature-focused instead of benefit-focused
- **1**: Unclear what the product actually does

**Cronicorn-Specific Success Indicators:**
- âœ… Uses one of the approved taglines/value props from marketing docs
- âœ… Emphasizes "adaptive" or "intelligent" scheduling
- âœ… Immediately differentiates from "traditional cron jobs"
- âœ… Avoids "AI magic" language (stays transparent)

**Reference Materials:**
- Primary tagline: "Intelligent job scheduling that adapts to your reality"
- Elevator pitch: "AI-powered job scheduler that automatically adapts to your system's reality"

---

### Criterion 2.2: Differentiation (Weight: 5 points)

**What to Evaluate:**
- Does it explain how Cronicorn is different from alternatives?
- Is the differentiation compelling and believable?
- Does it avoid negative competitor comparisons?

**Scoring Guide:**
- **5**: Clearly positions against "static schedulers" without naming competitors, emphasizes transparency
- **4**: Clear differentiation with good contrast
- **3**: Some differentiation but not compelling
- **2**: Weak or confusing positioning
- **1**: No differentiation or negative competitor bashing

**Cronicorn-Specific Success Indicators:**
- âœ… Contrasts static vs. adaptive scheduling
- âœ… Mentions "every decision explained" (transparency differentiator)
- âœ… Highlights min/max constraints (user control)
- âŒ Avoid: "Better than X" or competitor name-dropping

---

## ðŸŽ¬ Part 3: Product Demonstration (30-90 seconds)

### Criterion 3.1: Use Case Storytelling (Weight: 15 points)

**What to Evaluate:**
- Does the demo show a real, relatable scenario instead of generic features?
- Is the user journey clear and logical?
- Does it show before/after or problem/solution in action?

**Scoring Guide:**
- **5**: Demonstrates complete scenario that target audience recognizes immediately with clear before/after
  - Example: Flash sale scenario showing 5min â†’ 30sec adaptation â†’ back to 5min
- **4**: Good scenario with clear progression but minor gaps in storytelling
- **3**: Shows features in context but lacks narrative flow
- **2**: Feature tour without real-world context
- **1**: Random feature showcasing without coherent story

**Cronicorn-Specific Success Indicators:**
- âœ… Uses one of the documented use cases (DevOps monitoring, e-commerce, data pipelines)
- âœ… Shows the timeline visualization (the GIF in README demonstrates this well)
- âœ… Demonstrates complete workflow: detect â†’ adapt â†’ recover â†’ stabilize
- âœ… Makes AI reasoning visible ("Traffic surge detectedâ€”tightening to 30s")

**Recommended Use Case for Demo:**
Primary: DevOps monitoring with incident response (most universal)
Backup: E-commerce flash sale (more visual, shows dramatic adaptation)

---

### Criterion 3.2: Visual Clarity (Weight: 10 points)

**What to Evaluate:**
- Are screen recordings clear and easy to follow?
- Is text readable at typical viewing sizes?
- Are important UI elements highlighted or called out?
- Is pacing appropriate (not too fast or slow)?

**Scoring Guide:**
- **5**: Professional screen recordings with strategic highlights, annotations, smooth transitions
- **4**: Clear visuals with good pacing and mostly readable text
- **3**: Adequate clarity but some elements hard to see or too fast
- **2**: Poor visual quality, hard to follow, or confusing jumps
- **1**: Unclear screen recordings or disorienting presentation

**Cronicorn-Specific Success Indicators:**
- âœ… Shows dashboard timeline visualization clearly
- âœ… Highlights AI reasoning/explanations in UI
- âœ… Demonstrates both API and web UI (versatility)
- âœ… Uses zoom/highlight for important details
- âœ… Smooth transitions between screens (professional feel)

**Technical Specifications:**
- Minimum 1080p resolution
- Screen recordings at consistent zoom level
- Clear mouse movements (not erratic)
- Readable font sizes (16px+ for body text when shown)

---

### Criterion 3.3: Feature-to-Benefit Translation (Weight: 10 points)

**What to Evaluate:**
- For each feature shown, is the benefit immediately clear?
- Does narration focus on "what this means for you" not just "what this does"?
- Are technical capabilities translated to outcomes?

**Scoring Guide:**
- **5**: Every feature shown is immediately connected to concrete benefit with outcome metrics
  - Example: "AI tightens to 30 seconds [FEATURE] â†’ catch issues 10x faster [BENEFIT]"
- **4**: Most features clearly tied to benefits
- **3**: Some benefit translation but occasional feature-only descriptions
- **2**: Mostly feature descriptions without clear benefits
- **1**: Pure feature tour with no benefit context

**Cronicorn-Specific Success Indicators:**
- âœ… "Adaptive intervals" â†’ "Reduce alert fatigue by 80%"
- âœ… "AI reasoning visible" â†’ "Always know why schedules changed"
- âœ… "Min/max constraints" â†’ "You stay in control"
- âœ… "Auto-recovery workflows" â†’ "Fix issues before paging oncall"

**Key Benefit Metrics to Include:**
- 80% reduction in alert fatigue
- 10x faster issue resolution
- Zero schedule maintenance

---

### Criterion 3.4: Transparency & Trust Building (Weight: 10 points)

**What to Evaluate:**
- Does the demo show how AI makes decisions (not black box)?
- Are limitations or constraints mentioned?
- Does it build credibility through honesty?

**Scoring Guide:**
- **5**: Explicitly shows AI reasoning, mentions constraints, demonstrates user control mechanisms
- **4**: Good transparency with most trust elements present
- **3**: Some transparency but could be more explicit
- **2**: Vague about how AI works or overpromises
- **1**: Black box presentation or makes unrealistic claims

**Cronicorn-Specific Success Indicators:**
- âœ… Shows exact AI reasoning message: "Traffic surge detectedâ€”tightening monitoring to 30 seconds"
- âœ… Demonstrates min/max constraint enforcement
- âœ… Mentions that AI is optional: "Works perfectly without AI"
- âœ… Shows graceful degradation: "Baseline schedule continues if AI unavailable"
- âœ… Avoids: "AI magic," "just works," or hand-wavy explanations

**Brand Voice Alignment:**
This criterion is critical for Cronicorn's "Transparent" brand pillarâ€”we show our work, no black boxes.

---

## ðŸŽ¤ Part 4: Production Quality (Throughout)

### Criterion 4.1: Audio Quality (Weight: 5 points)

**What to Evaluate:**
- Is voiceover clear, professional, and easy to understand?
- Is audio level consistent throughout?
- Is pacing appropriate (not too fast or monotone)?

**Scoring Guide:**
- **5**: Professional voiceover with perfect clarity, engaging tone, appropriate pacing
- **4**: Clear audio with minor imperfections
- **3**: Understandable but amateur quality or pacing issues
- **2**: Distracting audio issues (echo, volume jumps, unclear speech)
- **1**: Poor audio quality that hinders comprehension

**Cronicorn-Specific Success Indicators:**
- âœ… Tone matches brand voice: "Senior engineer explaining to colleague"
- âœ… Conversational but confident (not corporate or overly casual)
- âœ… Technical terms pronounced correctly
- âŒ Avoid: Over-enthusiastic sales voice or monotone reading

---

### Criterion 4.2: Visual Polish (Weight: 5 points)

**What to Evaluate:**
- Are transitions smooth and professional?
- Is branding consistent (logo, colors, fonts)?
- Are animations purposeful (not gratuitous)?
- Does it look modern and trustworthy?

**Scoring Guide:**
- **5**: Polished production with branded elements, smooth transitions, purposeful motion
- **4**: Professional look with minor rough edges
- **3**: Adequate quality but feels amateur in places
- **2**: Distracting visual issues or inconsistent branding
- **1**: Poor production quality that undermines credibility

**Cronicorn-Specific Success Indicators:**
- âœ… Uses brand blue (#3B82F6) consistently
- âœ… Logo appears appropriately (start/end, not distracting)
- âœ… Matches modern developer tool aesthetic (Linear, Vercel, Stripe style)
- âœ… Subtle animations that enhance understanding
- âŒ Avoid: Over-designed motion graphics or stock footage

---

## ðŸ“ž Part 5: Call-to-Action (90-120 seconds)

### Criterion 5.1: CTA Clarity (Weight: 5 points)

**What to Evaluate:**
- Is the next step immediately obvious?
- Is there a single, clear primary action?
- Does it reduce friction (no credit card, free trial, etc.)?

**Scoring Guide:**
- **5**: Single, crystal clear CTA with friction reducers and compelling reason to act now
  - Example: "Start scheduling smarterâ€”free trial, no credit card required"
- **4**: Clear CTA with minor room for improvement
- **3**: CTA present but not compelling or slightly confusing
- **2**: Weak CTA or multiple competing actions
- **1**: No clear CTA or confusing next steps

**Cronicorn-Specific Success Indicators:**
- âœ… Primary CTA: "Get Early Access" or "Start Free Trial"
- âœ… Includes friction reducers: "No credit card required," "14-day free trial"
- âœ… Shows URL clearly: cronicorn.com or docs.cronicorn.com
- âœ… Optional secondary CTA: "View Documentation" or "See API Playground"

**Approved CTAs from Marketing Docs:**
- "Get Early Access"
- "Start Free Trial"
- "See It In Action"

---

### Criterion 5.2: Momentum & Urgency (Weight: 5 points)

**What to Evaluate:**
- Does the ending create desire to try the product now?
- Is there a reason not to wait?
- Does it end on a high note?

**Scoring Guide:**
- **5**: Creates genuine urgency through value reinforcement and clear benefit of acting now
- **4**: Good momentum with compelling ending
- **3**: Adequate close but lacks excitement
- **2**: Weak ending that loses momentum
- **1**: Abrupt ending or actively discourages immediate action

**Cronicorn-Specific Success Indicators:**
- âœ… Reinforces key benefit: "Stop getting paged at 3 AM"
- âœ… References ease: "Set up your first job in 5 minutes"
- âœ… Ends with confidence, not pleading
- âœ… Leaves viewer thinking "I should try this"

---

## ðŸ“ Part 6: Overall Effectiveness

### Criterion 6.1: Length & Pacing (Weight: 5 points)

**What to Evaluate:**
- Is total length within 90-120 second target?
- Does pacing feel appropriate throughout?
- Are any sections too rushed or too slow?

**Scoring Guide:**
- **5**: Perfect 90-120 second length with well-balanced pacing throughout
- **4**: Good length and pacing with minor sections that could be tightened
- **3**: Acceptable length but uneven pacing
- **2**: Too long (>150 sec) or too short (<60 sec), or very uneven pacing
- **1**: Length severely impacts effectiveness or pacing is confusing

**Research-Based Rationale:**
- 60-180 seconds is optimal for SaaS demos viewed independently
- 90-120 seconds hits sweet spot for first-time viewer engagement
- Videos under 3 minutes maintain highest completion rates

---

### Criterion 6.2: Target Audience Alignment (Weight: 5 points)

**What to Evaluate:**
- Would the target audience (DevOps/SRE engineers) find this compelling?
- Does it respect their technical competence?
- Does language and tone match their expectations?

**Scoring Guide:**
- **5**: Perfectly calibrated for technical audienceâ€”respects intelligence while being accessible
- **4**: Good audience fit with minor mismatches
- **3**: Somewhat generic or occasionally talks down/up to audience
- **2**: Poor audience targeting or condescending tone
- **1**: Wrong audience or actively alienating

**Cronicorn-Specific Success Indicators:**
- âœ… Uses technical terms correctly: "endpoints," "webhooks," "cron expressions," "adaptive intervals"
- âœ… References real workflows: "oncall," "paging," "incident response," "monitoring"
- âœ… Respects competence: Doesn't over-explain basics
- âœ… Shows technical depth: API examples, constraint configuration
- âŒ Avoid: "Magic," "simple," "easy" (can sound patronizing)

**Secondary Audiences:**
- Engineering Managers: Focus on team efficiency, cost reduction
- CTOs: Focus on scalability, competitive advantage

---

## ðŸ“‹ Scoring Summary Template

| Category | Criterion | Weight | Score (1-5) | Weighted |
|----------|-----------|--------|-------------|----------|
| **Hook & Problem** | 1.1 Immediate Value Hook | 10 | ___ | ___ |
| | 1.2 Problem Context | 5 | ___ | ___ |
| **Solution Intro** | 2.1 Value Prop Clarity | 10 | ___ | ___ |
| | 2.2 Differentiation | 5 | ___ | ___ |
| **Demonstration** | 3.1 Use Case Storytelling | 15 | ___ | ___ |
| | 3.2 Visual Clarity | 10 | ___ | ___ |
| | 3.3 Feature-to-Benefit | 10 | ___ | ___ |
| | 3.4 Transparency & Trust | 10 | ___ | ___ |
| **Production** | 4.1 Audio Quality | 5 | ___ | ___ |
| | 4.2 Visual Polish | 5 | ___ | ___ |
| **Call-to-Action** | 5.1 CTA Clarity | 5 | ___ | ___ |
| | 5.2 Momentum & Urgency | 5 | ___ | ___ |
| **Overall** | 6.1 Length & Pacing | 5 | ___ | ___ |
| | 6.2 Audience Alignment | 5 | ___ | ___ |
| **TOTAL** | | **100** | | **___/100** |

**Interpretation:**
- **90-100**: Exceptional - Production-ready, best-in-class demo
- **80-89**: Strong - Ready for production with minor polish
- **70-79**: Good - Solid foundation, needs targeted improvements
- **60-69**: Adequate - Functional but requires significant refinement
- **Below 60**: Needs Major Revision - Address fundamental issues before production

---

## ðŸŽ¬ Recommended Demo Structure (90-120 seconds)

### Section 1: Hook & Problem (0-15 seconds)
**Visual**: Quick montage or single powerful shot
**Narration**: "Getting paged at 3 AM for database blips that fix themselves in 30 seconds? Your health checks run every 5 minutesâ€”whether your API is on fire or humming along perfectly."
**On-screen text**: Optional problem stat

### Section 2: Solution Introduction (15-30 seconds)
**Visual**: Cronicorn dashboard/logo reveal
**Narration**: "Cronicorn is an AI-powered job scheduler that adapts to your system's reality. Instead of rigid schedules, it tightens monitoring during incidents and relaxes during recovery."
**On-screen text**: Value prop tagline

### Section 3: Use Case Demo (30-85 seconds)
**Visual**: Screen recording of complete scenario
**Narration**: Walk through specific use case showing:
- Baseline schedule running
- Issue detection / traffic surge
- AI reasoning visible: "Traffic surge detectedâ€”tightening to 30 seconds"
- Timeline showing adaptation
- Recovery and return to baseline
- Result: "80% fewer false alerts, 10x faster detection"

**On-screen highlights**: Call out AI reasoning, constraints, transparency

### Section 4: Key Differentiators (85-105 seconds)
**Visual**: Quick feature highlights
**Narration**: "Every decision explained. Min/max constraints keep you in control. Works perfectly with or without AI."
**On-screen text**: Bullet points of key benefits

### Section 5: Call-to-Action (105-120 seconds)
**Visual**: Dashboard with CTA overlay
**Narration**: "Stop getting paged at 3 AM. Start scheduling smarter."
**On-screen text**: "Get Early Access â€” cronicorn.com â€” No credit card required"

---

## ðŸŽ¯ Success Metrics (Post-Launch)

Once the demo video is deployed, track these metrics:

**Engagement Metrics:**
- View completion rate (Target: >65% for 90-120 sec video)
- Average watch time (Target: >75 seconds)
- Replay rate (Target: >10%)

**Conversion Metrics:**
- Landing page conversion rate with video vs. without (Target: +80% improvement)
- Click-through rate from video to CTA (Target: >15%)
- Trial sign-ups attributed to video (Track via UTM parameters)

**Qualitative Feedback:**
- User interviews: "What did you learn from the demo?"
- Support tickets: Does video reduce confusion questions?
- Social sharing: Are people organically sharing the video?

---

## ðŸ”„ Iteration Framework

After initial launch:

**Week 1-2: Gather Data**
- Watch session recordings of users viewing the demo
- Track drop-off points in video analytics
- Collect qualitative feedback from trial users

**Week 3-4: Analyze & Prioritize**
- Identify sections where viewers drop off
- Note frequently asked questions after viewing
- Prioritize improvements based on impact/effort

**Month 2: Test Variations**
- A/B test different hooks or CTAs
- Try alternate use case scenarios
- Test different video lengths

**Quarterly: Major Updates**
- Refresh demo as product evolves
- Update based on new marketing insights
- Incorporate successful customer stories

---

## ðŸ“š Reference Examples to Study

### Developer Tool Demos (Study These)
- **Vercel**: Clean, fast-paced, developer-focused
- **Linear**: Modern aesthetic, confident tone, problem-solution structure
- **Stripe**: Clear explanations, respects technical audience
- **Notion**: Complete workflow demonstration, approachable
- **Slack**: Casual conversation style, quick onboarding feel

### What Makes Them Successful
- **Problem-first approach**: Start with relatable pain
- **Real scenarios**: Not generic feature tours
- **Technical respect**: Don't oversimplify for engineers
- **Clarity over flash**: Professional but not over-produced
- **Clear value**: Benefit of each feature is obvious

### What to Avoid
- **Feature dumps**: Listing capabilities without context
- **Generic stock footage**: Feels inauthentic for dev tools
- **Corporate voice**: Too formal for developer audience
- **Hype language**: "Revolutionary," "game-changing," etc.
- **No clear CTA**: Leaving viewers unsure what to do next

---

## âœ… Pre-Production Checklist

Before creating the demo video, ensure:

**Content Planning:**
- [ ] Use case selected and validated with target users
- [ ] Script written and scored against rubric
- [ ] Key features to demonstrate identified (max 3-4)
- [ ] Benefits clearly articulated for each feature
- [ ] AI transparency moments scripted
- [ ] CTA and friction reducers finalized

**Production Preparation:**
- [ ] Demo environment set up with clean data
- [ ] Screen recordings planned and storyboarded
- [ ] Brand assets gathered (logo, colors, fonts)
- [ ] Voiceover artist selected (or AI voice tested)
- [ ] Timeline visualization created/ready to show
- [ ] Annotations and highlights planned

**Technical Requirements:**
- [ ] Recording software tested (1080p minimum)
- [ ] Audio equipment/setup tested
- [ ] Video editing software selected
- [ ] Export settings determined (format, resolution, compression)
- [ ] Hosting platform selected (YouTube, Vimeo, self-hosted)

**Brand Alignment:**
- [ ] Script reviewed against brand voice guidelines
- [ ] Messaging approved by marketing
- [ ] Technical accuracy verified by engineering
- [ ] Tone matches "senior engineer to colleague"
- [ ] Transparency requirements met (show AI reasoning)

---

## ðŸŽ“ Training: How to Use This Rubric

### For Video Creators:
1. **Script Phase**: Score your script draft against criteria before production
2. **Production**: Reference checklist to ensure all elements are captured
3. **Post-Production**: Self-evaluate rough cut and iterate before final
4. **Final Review**: Score final video and ensure >80/100 target

### For Reviewers:
1. **Independent Scoring**: Watch video and score each criterion individually
2. **Calibration**: Compare scores with other reviewers and discuss differences
3. **Prioritized Feedback**: Focus on lowest-scoring high-weight criteria first
4. **Actionable Notes**: Provide specific examples for improvements

### For Stakeholders:
1. **Quick Assessment**: Review total score and category breakdowns
2. **Focus Areas**: Identify which sections need most attention
3. **Go/No-Go**: Use 80/100 threshold for production readiness decision
4. **Iteration Planning**: Prioritize improvements based on weighted scores

---

## ðŸ“ž Questions & Revisions

This rubric should evolve based on:
- **User testing**: What actually converts viewers to trial users?
- **A/B testing**: Which approaches perform best?
- **Industry trends**: What are competitors doing successfully?
- **Product evolution**: As Cronicorn matures, demo should too

**Review quarterly** and update based on:
- Video performance metrics
- User feedback from trial sign-ups
- Changes to product positioning
- New features or capabilities

---

**Version:** 1.0
**Created:** 2026-01-06
**Last Updated:** 2026-01-06
**Owner:** Marketing/Growth Team
**Status:** Ready for Use

---

## ðŸ”— Related Documents

- [Marketing Overview](./overview.md) - Overall marketing strategy and positioning
- [Brand Voice Guidelines](./brand-voice.md) - Tone, style, and messaging framework
- [Copy Cheat Sheet](./copy-cheatsheet.md) - Quick reference for messaging
- [Landing Page Blueprints](./page-blueprints.md) - Website content structure
- [SEO Strategy](./seo-strategy.md) - Video SEO and distribution

---

**Key Takeaway**: This rubric emphasizes **problem-first storytelling**, **transparent AI demonstration**, and **technical respect** for the DevOps/SRE audience. A successful Cronicorn demo video should make engineers think "this solves my exact problem" within the first 15 seconds and show them exactly how within the next 75 seconds.

---

## ðŸŽ¥ SHOT-BY-SHOT PRODUCTION SCRIPT

**Use Case:** E-Commerce Flash Sale Demo (seeded data ready at `apps/migrator/src/seed.ts`)

**Total Runtime:** 105 seconds (1:45)

### Pre-Recording Setup

**Run the seed script:**
```bash
pnpm db:reset
pnpm db:migrate
pnpm tsx apps/migrator/src/seed.ts
```

**What this creates:**
- 1 job: "E-Commerce Flash Sale Demo" (all 11 endpoints)
- 18,566 runs over 7 days
- 6 AI analysis sessions with transparent reasoning
- Complete flash sale cycle: Day 6, 12:00-13:00 (25 hours ago from now)

**Dashboard navigation:**
- Open `/dashboard`
- Select job: "E-Commerce Flash Sale Demo"
- Timeline will show 7 days of data with visible spike at Day 6, 12:00-13:00

---

### SHOT 1: Hook & Problem (0-12 seconds)

**Visual:**
- Start on **dashboard homepage** showing the job dropdown
- Quick pan across timeline showing the flash sale spike (Day 6, 12:00-13:00)
- Zoom slightly into the critical phase (minutes 13-20) showing red/failed runs

**Narration:**
> "Your e-commerce site just launched a flash sale. Traffic jumped from 1,000 to 6,000 visitors per minute. Your monitoring? Still checking every 5 minutesâ€”missing the meltdown in real-time."

**On-screen text (3 seconds each):**
- "Traffic: 1,000 â†’ 6,000/min"
- "Health checks: Still every 5 minutes"
- "Issue detection: 10+ minutes late"

**What to capture:**
- Timeline visualization clearly showing baseline (green) vs critical (red) periods
- Visible clustering of runs during flash sale window
- Success rate drop visible in timeline (baseline: mostly green, critical: lots of red)

---

### SHOT 2: Solution Introduction (12-27 seconds)

**Visual:**
- Cut to **Cronicorn logo** briefly (2 seconds)
- Transition to **dashboard overview** showing the full 7-day timeline
- Highlight the **source attribution colors**:
  - Baseline interval: Green dots (days 1-5)
  - AI interval: Blue dots (flash sale window)
  - AI one-shot: Purple dots (recovery/alert actions)
  - Clamped-min: Orange dots (critical phase)

**Narration:**
> "Cronicorn is an AI-powered job scheduler that adapts to your system's reality. When your site experiences a traffic surge, it automatically tightens monitoringâ€”without you lifting a finger. Every decision is transparent. You stay in control."

**On-screen text:**
- "AI adapts schedules in real-time"
- "Every decision explained"
- "You control min/max constraints"

**What to capture:**
- Full timeline showing 7-day view (establish baseline credibility)
- Color legend visible (if UI shows it) or add annotation explaining colors
- Smooth pan from left (Day 1 baseline) to center (Day 6 flash sale) to right (Day 7 recovery)

---

### SHOT 3A: Baseline Credibility (27-35 seconds)

**Visual:**
- Zoom into **Days 1-5** on timeline
- Show **Traffic Monitor endpoint** details:
  - Name: "Traffic Monitor"
  - Baseline interval: 60,000ms (1 minute)
  - Runs: Green dots, evenly spaced
  - Success rate: 95-98%

**Narration:**
> "For five days before the sale, Cronicorn establishes a baseline. Traffic Monitor checks every minute. Orders, inventoryâ€”all on predictable schedules. 98% success rate. Everything's calm."

**On-screen highlights:**
- Circle the baseline interval: "1 minute baseline"
- Arrow pointing to response body metrics: "1,000 visitors/min, 800ms page load"
- Success indicators: "98% success"

**What to capture:**
- Click on **Traffic Monitor** endpoint to show details panel
- Scroll to show **response body** with metrics:
  ```json
  {
    "traffic": 1000,
    "ordersPerMin": 40,
    "pageLoadMs": 800,
    "inventoryLagMs": 100,
    "dbQueryMs": 120
  }
  ```
- Show latest run timestamp from Days 1-5

---

### SHOT 3B: Flash Sale Surge Detection (35-50 seconds)

**Visual:**
- Pan timeline to **Day 6, 12:05-12:08** (Surge phase)
- Zoom into **minute 6** specifically (12:06)
- Show **AI Session #2** panel opening
- Highlight the tool call: `propose_interval(30000, 60)`

**Narration:**
> "Then the flash sale hits. Minute 6: traffic spikes to 5,000 per minute. Cronicorn's AI notices immediatelyâ€”and here's the key: it shows you exactly what it's thinking."

**AI Reasoning visible on screen:**
> "Traffic surge from 1,000 to 5,100 visitors/min detected. Tightening health check intervals to 30 seconds for proactive monitoring during flash sale peak."

**On-screen highlights:**
- Circle the **AI Session** entry at **12:06**
- Highlight the tool call: `propose_interval(30000, 60)`
- Arrow showing: "1 minute â†’ 30 seconds (2x faster)"
- Show response body changing: "traffic: 5100, pageLoadMs: 1850"

**What to capture:**
- Click on **AI Session** from timeline (if clickable) or navigate to AI sessions panel
- Show full reasoning text
- Show **tool calls** section with `get_latest_response`, `get_response_history`, `propose_interval`
- Token usage: 1,250 tokens
- Duration: 420ms

---

### SHOT 3C: Critical Phase & Multi-Tier Response (50-68 seconds)

**Visual:**
- Pan timeline to **Day 6, 12:13-12:20** (Critical phase)
- Show timeline changing colors: Blue (AI interval) â†’ Orange (clamped-min) â†’ Purple (one-shot)
- Split screen or quick cuts between:
  1. **Traffic Monitor** - interval clamped to 20 seconds (min constraint)
  2. **Slow Page Analyzer** - activated (was paused, now running)
  3. **Cache Warmup** - purple one-shot execution
  4. **Emergency Oncall Page** - purple one-shot execution

**Narration:**
> "Minute 13: things get critical. 6,000 visitors, page load at 4.5 seconds, success rate drops to 60%. Watch what happens. AI tightens monitoring to the minimumâ€”20 seconds. It activates diagnostics that were paused. It triggers cache warmup. And when orders drop below 120 per minute? It pages oncall."

**On-screen highlights:**
- **Traffic Monitor**: "Interval: 20s (min constraint enforced)"
- **Slow Page Analyzer**: "Status: ACTIVATED (was paused)"
- **Cache Warmup**: "Triggered: One-shot recovery action"
- **Emergency Oncall**: "Paged oncall: Critical threshold"

**What to capture:**
- Show **AI Session #3** at **12:15**:
  - Tool call: `pause_until(null)` - unpausing Slow Page Analyzer
  - Tool call: `propose_next_time(5000, 5)` - cache warmup in 5 seconds
  - Reasoning: "CRITICAL: Page load times at 4600ms..."
- Show endpoint list with **4 tiers** visible:
  - Health (3): Continuous, tightened
  - Investigation (2): Now active (was paused)
  - Recovery (2): One-shot actions (purple dots)
  - Alert (4): Escalation (purple dots)
- Response body metrics at critical peak:
  ```json
  {
    "traffic": 6000,
    "pageLoadMs": 4500,
    "ordersPerMin": 120,
    "inventoryLagMs": 600,
    "dbQueryMs": 1200
  }
  ```

---

### SHOT 3D: Recovery & Hint Expiration (68-85 seconds)

**Visual:**
- Pan timeline to **Day 6, 12:21-12:39** (Recovery phase)
- Show colors shifting back: Orange â†’ Blue â†’ Green
- Show metrics improving in response bodies
- Jump to **Day 6, 13:05** - hint expiration moment

**Narration:**
> "Minute 21: recovery begins. Traffic drops, page load improves. AI confirms the recovery and loosens back to baseline. By 13:05â€”exactly 60 minutes laterâ€”all AI hints expire automatically. The system returns to normal. No manual intervention required."

**On-screen highlights:**
- **Minute 28** (12:28): "AI Session #4: Recovery confirmed"
- Show tool call: `propose_interval(60000, 30)` - back to 1 minute
- **Minute 65** (13:05): "AI Session #5: Hints expired"
- Show reasoning: "All AI hints expired (60-min TTL). Baseline schedules fully resumed."
- **Day 7, 08:00**: "AI Session #6: Stability confirmed"

**What to capture:**
- Show **AI Session #4** reasoning: "Recovery confirmed. Traffic declining to 1,400/min, page load improved to 1,050ms..."
- Show timeline return to green (baseline-interval source)
- Show **AI Session #5** with TTL expiration message
- Pan to Day 7 showing normal baseline pattern restored

**Metrics to show:**
- Recovery phase: traffic: 1500, pageLoadMs: 1100, ordersPerMin: 50
- Post-expiration: traffic: 1050, pageLoadMs: 820, ordersPerMin: 42

---

### SHOT 4: Key Differentiators (85-95 seconds)

**Visual:**
- Cut to **endpoint configuration view** showing:
  - **Traffic Monitor** endpoint details
  - Min interval: 20,000ms (20 seconds)
  - Max interval: 300,000ms (5 minutes)
  - Baseline interval: 60,000ms (1 minute)
- Show **AI Sessions** list panel with all 6 sessions visible
- Quick highlight of **source attribution** legend

**Narration:**
> "Three things make this different: Oneâ€”every decision is transparent. You saw exactly why AI tightened to 30 seconds. Twoâ€”you control the boundaries. Min 20 seconds, max 5 minutes. AI operates within your constraints. Threeâ€”it works perfectly without AI. Baseline schedules continue if AI is unavailable."

**On-screen text:**
- "âœ“ Every decision explained"
- "âœ“ Min/max constraints enforced"
- "âœ“ Works without AI"

**What to capture:**
- Show **min/max constraint fields** in endpoint configuration
- Show **all 6 AI sessions** in list view with timestamps
- Show **baseline interval** field highlighted
- Optional: Show AI toggle or configuration (if exists in UI)

---

### SHOT 5: Call-to-Action (95-105 seconds)

**Visual:**
- Cut back to **dashboard overview** with full 7-day timeline
- Zoom out slightly to show complete story: baseline â†’ surge â†’ recovery
- Fade to **Cronicorn logo** on clean background
- **CTA overlay** appears

**Narration:**
> "Stop missing critical issues because your schedules can't keep up. Cronicorn adapts in real-time. Start scheduling smarter."

**On-screen text (large, clear):**
```
Get Early Access
cronicorn.com
No credit card required
```

**Secondary text (smaller, bottom):**
```
14-day free trial â€¢ Set up in 5 minutes
```

**What to capture:**
- Final pan of complete timeline showing the full adaptation cycle
- Clean fade to logo and CTA
- CTA visible for 5-8 seconds before fade out

---

### POST-PRODUCTION CHECKLIST

**Annotations to Add:**
- [ ] **Timeline colors legend**: Add persistent legend explaining source types
  - Green = Baseline interval
  - Blue = AI-adjusted interval
  - Purple = AI one-shot action
  - Orange = Clamped to min/max
- [ ] **Metrics callouts**: Add animated boxes around key metrics when mentioned
- [ ] **AI reasoning highlights**: Box or highlight AI session reasoning text for readability
- [ ] **Timeline scrubber**: Optional: Add timestamp indicator showing current moment being discussed
- [ ] **Endpoint tier badges**: Add visual indicators for Health/Investigation/Recovery/Alert tiers

**Transitions:**
- Smooth pan/zoom on timeline (no cuts within timeline shots)
- Quick fade transitions between major sections (1-2 frames)
- No flashy effectsâ€”keep professional/developer tool aesthetic

**Audio:**
- Narration: "Senior engineer explaining to colleague" tone
- Background music: Optional subtle ambient (if used, keep very low volume)
- Sound effects: Minimalâ€”only if needed for emphasis (e.g., alert sound on oncall page)

**Text overlays:**
- Font: Sans-serif (Inter, SF Pro, or similar)
- Size: Large enough to read at 1080p on laptop (minimum 24px equivalent)
- Colors: Brand blue (#3B82F6) for highlights, white for text
- Duration: 3-5 seconds per text overlay (long enough to read twice)

**Branding:**
- Logo appears: Start (2 seconds) and End (5-8 seconds)
- Brand colors: Use blue accent consistently
- No heavy branding during demo (keep focus on product)

---

### SEED DATA REFERENCE (Quick Lookup)

**Timeline Structure:**
- **Days 1-5**: Baseline establishment (5 days before sale)
- **Day 6, 12:00-13:00**: Flash sale event (60 minutes)
- **Day 7**: Post-event stabilization

**Flash Sale Phases (Day 6, 12:00-13:00):**
| Phase | Minutes | Traffic | Success | Page Load | AI Action |
|-------|---------|---------|---------|-----------|-----------|
| Baseline | 0-4 | 1,000/min | 98% | 800ms | None |
| Surge | 5-8 | 5,000/min | 92% | 1,800ms | â†’ 30s intervals |
| Strain | 9-12 | 5,500/min | 85% | 3,200ms | Activate diagnostics |
| Critical | 13-20 | 6,000/min | 60% | 4,500ms | Cache + Oncall |
| Recovery | 21-39 | 1,500/min | 95% | 1,100ms | â†’ Baseline |
| Post-Sale | 40+ | 1,050/min | 98% | 820ms | Hints expire |

**AI Sessions (6 total):**
1. **11:45** (Pre-sale): "Normal patterns, no action needed"
2. **12:06** (Surge): `propose_interval(30000)` - "Tightening to 30s"
3. **12:15** (Critical): `pause_until(null)`, `propose_next_time` - "Emergency recovery"
4. **12:28** (Recovery): `propose_interval(60000)` - "Back to baseline"
5. **13:05** (Expiration): "All hints expired (60-min TTL)"
6. **Day 7, 08:00** (Stability): "Post-event stability confirmed"

**Endpoints to Feature:**
- **Traffic Monitor** (Health): 10,112 runs, 1min baseline â†’ 20s min
- **Slow Page Analyzer** (Investigation): 8 runs, activated during strain
- **Cache Warmup** (Recovery): 2 runs, one-shot actions with 10min cooldown
- **Emergency Oncall Page** (Alert): 1 run, 2-hour cooldown

**Run Counts (Total: 18,566):**
- Health tier: 18,541 runs (continuous monitoring)
- Investigation: 14 runs (flash sale only)
- Recovery: 4 runs (one-shot with cooldowns)
- Alert: 7 runs (one-shot with cooldowns)

**Source Distribution (During Flash Sale):**
- Baseline: 15-40% (depending on phase)
- AI Interval: 60-85% (majority during active adaptation)
- AI One-shot: 5-10% (recovery/alert actions)
- Clamped-min: ~5% (critical phase)

---

### RECORDING TIPS

**Screen Recording Settings:**
- Resolution: 1920x1080 (1080p minimum)
- Frame rate: 30fps (60fps if smooth panning is critical)
- Software: QuickTime (macOS), OBS Studio, or similar
- Cursor: Smooth, deliberate movements (not erratic)
- Hide unnecessary UI: Browser bookmarks bar, dock (if possible)

**Dashboard Prep:**
- Clear any notification badges or distractions
- Ensure browser is full-screen or zoomed appropriately
- Have all panels/views pre-navigated (practice run first)
- Consider multiple takes per shot (choose best later)

**Timing the Recording:**
- Seed data is relative to "NOW" so timing is consistent
- Flash sale always appears at Day 6, 12:00-13:00 (25 hours ago)
- Run seed script fresh before recording to ensure clean data

**Multiple Takes Strategy:**
- Record each SHOT separately (easier to edit)
- Record 2-3 takes of each shot
- Keep recording even if you make small mistakes (fix in post)
- Record timeline pans slowly (speed up in post if needed)

---

**Script Version:** 1.0
**Based on Seed Data:** `apps/migrator/src/seed.ts` (commit: [pending])
**Ready for:** Production recording
**Estimated Edit Time:** 3-5 hours (depending on polish level)
