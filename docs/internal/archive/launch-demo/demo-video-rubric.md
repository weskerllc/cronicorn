# Cronicorn Demo Video Rubric

**Purpose:** Evaluation framework for creating and assessing a conversion-focused product demo video that introduces new users to Cronicorn for the first time.

**Target Length:** 90-120 seconds (optimal for first-time viewers based on 2025 industry data)

**Primary Goal:** Convert visitors into trial users by clearly demonstrating value and increasing product understanding

---

## üìä Scoring System

Each criterion is scored 1-5:
- **5** = Exceptional - Best-in-class execution
- **4** = Strong - Meets all requirements with polish
- **3** = Adequate - Functional but room for improvement
- **2** = Weak - Missing key elements or poorly executed
- **1** = Poor - Needs major revision

**Target Score:** 80/100 (4.0 average) for production-ready demo

---

## üéØ Part 1: Hook & Problem Statement (0-15 seconds)

### Criterion 1.1: Immediate Value Hook (Weight: 10 points)

**What to Evaluate:**
- Does the first 3 seconds grab attention with a relatable problem?
- Is the hook specific to the target audience (DevOps/SRE engineers)?
- Does it create curiosity without giving everything away?

**Scoring Guide:**
- **5**: Opens with visceral, relatable pain point that makes target audience immediately think "that's me"
  - Example: "Getting paged at 3 AM for database blips that fix themselves in 30 seconds"
- **4**: Clear problem statement that resonates with target audience
- **3**: Generic problem statement that could apply to any scheduler
- **2**: Weak hook that takes too long to get to the point
- **1**: No clear hook or starts with features instead of problems

**Cronicorn-Specific Success Indicators:**
- ‚úÖ References alert fatigue, firefighting, or rigid scheduling
- ‚úÖ Uses language from brand voice (human, relatable, technical)
- ‚úÖ Immediately signals "this is for engineers" not generic marketing

**Examples:**
- ‚úÖ **Strong**: "Your health checks run every 5 minutes‚Äîwhether your API is on fire or humming along perfectly."
- ‚ùå **Weak**: "Cronicorn is an advanced AI-powered scheduling platform..."

---

### Criterion 1.2: Problem Context (Weight: 5 points)

**What to Evaluate:**
- Does it quickly establish why static scheduling is limiting?
- Is the pain quantified or made tangible?
- Does it avoid over-explaining?

**Scoring Guide:**
- **5**: Provides just enough context to establish credibility without dwelling on problems
- **4**: Clear problem context with 1-2 specific examples
- **3**: Generic problem description without specificity
- **2**: Either too vague or too lengthy
- **1**: Skips problem context entirely or confuses the viewer

**Cronicorn-Specific Success Indicators:**
- ‚úÖ Shows understanding of real DevOps workflows
- ‚úÖ References concrete scenarios (traffic surges, incidents, rate limits)
- ‚úÖ Ties to measurable outcomes (80% alert reduction, 10x faster resolution)

---

## üí° Part 2: Solution Introduction (15-30 seconds)

### Criterion 2.1: Value Proposition Clarity (Weight: 10 points)

**What to Evaluate:**
- Is the solution immediately clear and different from competitors?
- Does it communicate the "adaptive" benefit without jargon?
- Can a viewer explain what Cronicorn does after this section?

**Scoring Guide:**
- **5**: Crystal clear one-sentence value prop that differentiates from traditional schedulers
  - Example: "Cronicorn is an AI job scheduler that adapts to your system's reality"
- **4**: Clear value prop with minor room for simplification
- **3**: Understandable but requires careful listening
- **2**: Confusing or feature-focused instead of benefit-focused
- **1**: Unclear what the product actually does

**Cronicorn-Specific Success Indicators:**
- ‚úÖ Uses one of the approved taglines/value props from marketing docs
- ‚úÖ Emphasizes "adaptive" or "intelligent" scheduling
- ‚úÖ Immediately differentiates from "traditional cron jobs"
- ‚úÖ Avoids "AI magic" language (stays transparent)

**Reference Materials:**
- Primary tagline: "Intelligent job scheduling that adapts to your reality"
- Elevator pitch: "AI-powered job scheduler that automatically adapts to your system's reality"

---

### Criterion 2.2: Differentiation (Weight: 5 points)

**What to Evaluate:**
- Does it explain how Cronicorn is different from alternatives?
- Is the differentiation compelling and believable?
- Does it avoid negative competitor comparisons?

**Scoring Guide:**
- **5**: Clearly positions against "static schedulers" without naming competitors, emphasizes transparency
- **4**: Clear differentiation with good contrast
- **3**: Some differentiation but not compelling
- **2**: Weak or confusing positioning
- **1**: No differentiation or negative competitor bashing

**Cronicorn-Specific Success Indicators:**
- ‚úÖ Contrasts static vs. adaptive scheduling
- ‚úÖ Mentions "every decision explained" (transparency differentiator)
- ‚úÖ Highlights min/max constraints (user control)
- ‚ùå Avoid: "Better than X" or competitor name-dropping

---

## üé¨ Part 3: Product Demonstration (30-90 seconds)

### Criterion 3.1: Use Case Storytelling (Weight: 15 points)

**What to Evaluate:**
- Does the demo show a real, relatable scenario instead of generic features?
- Is the user journey clear and logical?
- Does it show before/after or problem/solution in action?

**Scoring Guide:**
- **5**: Demonstrates complete scenario that target audience recognizes immediately with clear before/after
  - Example: Flash sale scenario showing 5min ‚Üí 30sec adaptation ‚Üí back to 5min
- **4**: Good scenario with clear progression but minor gaps in storytelling
- **3**: Shows features in context but lacks narrative flow
- **2**: Feature tour without real-world context
- **1**: Random feature showcasing without coherent story

**Cronicorn-Specific Success Indicators:**
- ‚úÖ Uses one of the documented use cases (DevOps monitoring, e-commerce, data pipelines)
- ‚úÖ Shows the timeline visualization (the GIF in README demonstrates this well)
- ‚úÖ Demonstrates complete workflow: detect ‚Üí adapt ‚Üí recover ‚Üí stabilize
- ‚úÖ Makes AI reasoning visible ("Traffic surge detected‚Äîtightening to 30s")

**Recommended Use Case for Demo:**
Primary: DevOps monitoring with incident response (most universal)
Backup: E-commerce flash sale (more visual, shows dramatic adaptation)

---

### Criterion 3.2: Visual Clarity (Weight: 10 points)

**What to Evaluate:**
- Are screen recordings clear and easy to follow?
- Is text readable at typical viewing sizes?
- Are important UI elements highlighted or called out?
- Is pacing appropriate (not too fast or slow)?

**Scoring Guide:**
- **5**: Professional screen recordings with strategic highlights, annotations, smooth transitions
- **4**: Clear visuals with good pacing and mostly readable text
- **3**: Adequate clarity but some elements hard to see or too fast
- **2**: Poor visual quality, hard to follow, or confusing jumps
- **1**: Unclear screen recordings or disorienting presentation

**Cronicorn-Specific Success Indicators:**
- ‚úÖ Shows dashboard timeline visualization clearly
- ‚úÖ Highlights AI reasoning/explanations in UI
- ‚úÖ Demonstrates both API and web UI (versatility)
- ‚úÖ Uses zoom/highlight for important details
- ‚úÖ Smooth transitions between screens (professional feel)

**Technical Specifications:**
- Minimum 1080p resolution
- Screen recordings at consistent zoom level
- Clear mouse movements (not erratic)
- Readable font sizes (16px+ for body text when shown)

---

### Criterion 3.3: Feature-to-Benefit Translation (Weight: 10 points)

**What to Evaluate:**
- For each feature shown, is the benefit immediately clear?
- Does narration focus on "what this means for you" not just "what this does"?
- Are technical capabilities translated to outcomes?

**Scoring Guide:**
- **5**: Every feature shown is immediately connected to concrete benefit with outcome metrics
  - Example: "AI tightens to 30 seconds [FEATURE] ‚Üí catch issues 10x faster [BENEFIT]"
- **4**: Most features clearly tied to benefits
- **3**: Some benefit translation but occasional feature-only descriptions
- **2**: Mostly feature descriptions without clear benefits
- **1**: Pure feature tour with no benefit context

**Cronicorn-Specific Success Indicators:**
- ‚úÖ "Adaptive intervals" ‚Üí "Reduce alert fatigue by 80%"
- ‚úÖ "AI reasoning visible" ‚Üí "Always know why schedules changed"
- ‚úÖ "Min/max constraints" ‚Üí "You stay in control"
- ‚úÖ "Auto-recovery workflows" ‚Üí "Fix issues before paging oncall"

**Key Benefit Metrics to Include:**
- 80% reduction in alert fatigue
- 10x faster issue resolution
- Zero schedule maintenance

---

### Criterion 3.4: Transparency & Trust Building (Weight: 10 points)

**What to Evaluate:**
- Does the demo show how AI makes decisions (not black box)?
- Are limitations or constraints mentioned?
- Does it build credibility through honesty?

**Scoring Guide:**
- **5**: Explicitly shows AI reasoning, mentions constraints, demonstrates user control mechanisms
- **4**: Good transparency with most trust elements present
- **3**: Some transparency but could be more explicit
- **2**: Vague about how AI works or overpromises
- **1**: Black box presentation or makes unrealistic claims

**Cronicorn-Specific Success Indicators:**
- ‚úÖ Shows exact AI reasoning message: "Traffic surge detected‚Äîtightening monitoring to 30 seconds"
- ‚úÖ Demonstrates min/max constraint enforcement
- ‚úÖ Mentions that AI is optional: "Works perfectly without AI"
- ‚úÖ Shows graceful degradation: "Baseline schedule continues if AI unavailable"
- ‚úÖ Avoids: "AI magic," "just works," or hand-wavy explanations

**Brand Voice Alignment:**
This criterion is critical for Cronicorn's "Transparent" brand pillar‚Äîwe show our work, no black boxes.

---

## üé§ Part 4: Production Quality (Throughout)

### Criterion 4.1: Audio Quality (Weight: 5 points)

**What to Evaluate:**
- Is voiceover clear, professional, and easy to understand?
- Is audio level consistent throughout?
- Is pacing appropriate (not too fast or monotone)?

**Scoring Guide:**
- **5**: Professional voiceover with perfect clarity, engaging tone, appropriate pacing
- **4**: Clear audio with minor imperfections
- **3**: Understandable but amateur quality or pacing issues
- **2**: Distracting audio issues (echo, volume jumps, unclear speech)
- **1**: Poor audio quality that hinders comprehension

**Cronicorn-Specific Success Indicators:**
- ‚úÖ Tone matches brand voice: "Senior engineer explaining to colleague"
- ‚úÖ Conversational but confident (not corporate or overly casual)
- ‚úÖ Technical terms pronounced correctly
- ‚ùå Avoid: Over-enthusiastic sales voice or monotone reading

---

### Criterion 4.2: Visual Polish (Weight: 5 points)

**What to Evaluate:**
- Are transitions smooth and professional?
- Is branding consistent (logo, colors, fonts)?
- Are animations purposeful (not gratuitous)?
- Does it look modern and trustworthy?

**Scoring Guide:**
- **5**: Polished production with branded elements, smooth transitions, purposeful motion
- **4**: Professional look with minor rough edges
- **3**: Adequate quality but feels amateur in places
- **2**: Distracting visual issues or inconsistent branding
- **1**: Poor production quality that undermines credibility

**Cronicorn-Specific Success Indicators:**
- ‚úÖ Uses brand blue (#3B82F6) consistently
- ‚úÖ Logo appears appropriately (start/end, not distracting)
- ‚úÖ Matches modern developer tool aesthetic (Linear, Vercel, Stripe style)
- ‚úÖ Subtle animations that enhance understanding
- ‚ùå Avoid: Over-designed motion graphics or stock footage

---

## üìû Part 5: Call-to-Action (90-120 seconds)

### Criterion 5.1: CTA Clarity (Weight: 5 points)

**What to Evaluate:**
- Is the next step immediately obvious?
- Is there a single, clear primary action?
- Does it reduce friction (no credit card, free trial, etc.)?

**Scoring Guide:**
- **5**: Single, crystal clear CTA with friction reducers and compelling reason to act now
  - Example: "Start scheduling smarter‚Äîfree trial, no credit card required"
- **4**: Clear CTA with minor room for improvement
- **3**: CTA present but not compelling or slightly confusing
- **2**: Weak CTA or multiple competing actions
- **1**: No clear CTA or confusing next steps

**Cronicorn-Specific Success Indicators:**
- ‚úÖ Primary CTA: "Get Early Access" or "Start Free Trial"
- ‚úÖ Includes friction reducers: "No credit card required," "14-day free trial"
- ‚úÖ Shows URL clearly: cronicorn.com or docs.cronicorn.com
- ‚úÖ Optional secondary CTA: "View Documentation" or "See API Playground"

**Approved CTAs from Marketing Docs:**
- "Get Early Access"
- "Start Free Trial"
- "See It In Action"

---

### Criterion 5.2: Momentum & Urgency (Weight: 5 points)

**What to Evaluate:**
- Does the ending create desire to try the product now?
- Is there a reason not to wait?
- Does it end on a high note?

**Scoring Guide:**
- **5**: Creates genuine urgency through value reinforcement and clear benefit of acting now
- **4**: Good momentum with compelling ending
- **3**: Adequate close but lacks excitement
- **2**: Weak ending that loses momentum
- **1**: Abrupt ending or actively discourages immediate action

**Cronicorn-Specific Success Indicators:**
- ‚úÖ Reinforces key benefit: "Stop getting paged at 3 AM"
- ‚úÖ References ease: "Set up your first job in 5 minutes"
- ‚úÖ Ends with confidence, not pleading
- ‚úÖ Leaves viewer thinking "I should try this"

---

## üìè Part 6: Overall Effectiveness

### Criterion 6.1: Length & Pacing (Weight: 5 points)

**What to Evaluate:**
- Is total length within 90-120 second target?
- Does pacing feel appropriate throughout?
- Are any sections too rushed or too slow?

**Scoring Guide:**
- **5**: Perfect 90-120 second length with well-balanced pacing throughout
- **4**: Good length and pacing with minor sections that could be tightened
- **3**: Acceptable length but uneven pacing
- **2**: Too long (>150 sec) or too short (<60 sec), or very uneven pacing
- **1**: Length severely impacts effectiveness or pacing is confusing

**Research-Based Rationale:**
- 60-180 seconds is optimal for SaaS demos viewed independently
- 90-120 seconds hits sweet spot for first-time viewer engagement
- Videos under 3 minutes maintain highest completion rates

---

### Criterion 6.2: Target Audience Alignment (Weight: 5 points)

**What to Evaluate:**
- Would the target audience (DevOps/SRE engineers) find this compelling?
- Does it respect their technical competence?
- Does language and tone match their expectations?

**Scoring Guide:**
- **5**: Perfectly calibrated for technical audience‚Äîrespects intelligence while being accessible
- **4**: Good audience fit with minor mismatches
- **3**: Somewhat generic or occasionally talks down/up to audience
- **2**: Poor audience targeting or condescending tone
- **1**: Wrong audience or actively alienating

**Cronicorn-Specific Success Indicators:**
- ‚úÖ Uses technical terms correctly: "endpoints," "webhooks," "cron expressions," "adaptive intervals"
- ‚úÖ References real workflows: "oncall," "paging," "incident response," "monitoring"
- ‚úÖ Respects competence: Doesn't over-explain basics
- ‚úÖ Shows technical depth: API examples, constraint configuration
- ‚ùå Avoid: "Magic," "simple," "easy" (can sound patronizing)

**Secondary Audiences:**
- Engineering Managers: Focus on team efficiency, cost reduction
- CTOs: Focus on scalability, competitive advantage

---

## üìã Scoring Summary Template

| Category | Criterion | Weight | Score (1-5) | Weighted |
|----------|-----------|--------|-------------|----------|
| **Hook & Problem** | 1.1 Immediate Value Hook | 10 | ___ | ___ |
| | 1.2 Problem Context | 5 | ___ | ___ |
| **Solution Intro** | 2.1 Value Prop Clarity | 10 | ___ | ___ |
| | 2.2 Differentiation | 5 | ___ | ___ |
| **Demonstration** | 3.1 Use Case Storytelling | 15 | ___ | ___ |
| | 3.2 Visual Clarity | 10 | ___ | ___ |
| | 3.3 Feature-to-Benefit | 10 | ___ | ___ |
| | 3.4 Transparency & Trust | 10 | ___ | ___ |
| **Production** | 4.1 Audio Quality | 5 | ___ | ___ |
| | 4.2 Visual Polish | 5 | ___ | ___ |
| **Call-to-Action** | 5.1 CTA Clarity | 5 | ___ | ___ |
| | 5.2 Momentum & Urgency | 5 | ___ | ___ |
| **Overall** | 6.1 Length & Pacing | 5 | ___ | ___ |
| | 6.2 Audience Alignment | 5 | ___ | ___ |
| **TOTAL** | | **100** | | **___/100** |

**Interpretation:**
- **90-100**: Exceptional - Production-ready, best-in-class demo
- **80-89**: Strong - Ready for production with minor polish
- **70-79**: Good - Solid foundation, needs targeted improvements
- **60-69**: Adequate - Functional but requires significant refinement
- **Below 60**: Needs Major Revision - Address fundamental issues before production

---

## üé¨ Recommended Demo Structure (90-120 seconds)

### Section 1: Hook & Problem (0-15 seconds)
**Visual**: Quick montage or single powerful shot
**Narration**: "Getting paged at 3 AM for database blips that fix themselves in 30 seconds? Your health checks run every 5 minutes‚Äîwhether your API is on fire or humming along perfectly."
**On-screen text**: Optional problem stat

### Section 2: Solution Introduction (15-30 seconds)
**Visual**: Cronicorn dashboard/logo reveal
**Narration**: "Cronicorn is an AI-powered job scheduler that adapts to your system's reality. Instead of rigid schedules, it tightens monitoring during incidents and relaxes during recovery."
**On-screen text**: Value prop tagline

### Section 3: Use Case Demo (30-85 seconds)
**Visual**: Screen recording of complete scenario
**Narration**: Walk through specific use case showing:
- Baseline schedule running
- Issue detection / traffic surge
- AI reasoning visible: "Traffic surge detected‚Äîtightening to 30 seconds"
- Timeline showing adaptation
- Recovery and return to baseline
- Result: "80% fewer false alerts, 10x faster detection"

**On-screen highlights**: Call out AI reasoning, constraints, transparency

### Section 4: Key Differentiators (85-105 seconds)
**Visual**: Quick feature highlights
**Narration**: "Every decision explained. Min/max constraints keep you in control. Works perfectly with or without AI."
**On-screen text**: Bullet points of key benefits

### Section 5: Call-to-Action (105-120 seconds)
**Visual**: Dashboard with CTA overlay
**Narration**: "Stop getting paged at 3 AM. Start scheduling smarter."
**On-screen text**: "Get Early Access ‚Äî cronicorn.com ‚Äî No credit card required"

---

## üéØ Success Metrics (Post-Launch)

Once the demo video is deployed, track these metrics:

**Engagement Metrics:**
- View completion rate (Target: >65% for 90-120 sec video)
- Average watch time (Target: >75 seconds)
- Replay rate (Target: >10%)

**Conversion Metrics:**
- Landing page conversion rate with video vs. without (Target: +80% improvement)
- Click-through rate from video to CTA (Target: >15%)
- Trial sign-ups attributed to video (Track via UTM parameters)

**Qualitative Feedback:**
- User interviews: "What did you learn from the demo?"
- Support tickets: Does video reduce confusion questions?
- Social sharing: Are people organically sharing the video?

---

## üîÑ Iteration Framework

After initial launch:

**Week 1-2: Gather Data**
- Watch session recordings of users viewing the demo
- Track drop-off points in video analytics
- Collect qualitative feedback from trial users

**Week 3-4: Analyze & Prioritize**
- Identify sections where viewers drop off
- Note frequently asked questions after viewing
- Prioritize improvements based on impact/effort

**Month 2: Test Variations**
- A/B test different hooks or CTAs
- Try alternate use case scenarios
- Test different video lengths

**Quarterly: Major Updates**
- Refresh demo as product evolves
- Update based on new marketing insights
- Incorporate successful customer stories

---

## üìö Reference Examples to Study

### Developer Tool Demos (Study These)
- **Vercel**: Clean, fast-paced, developer-focused
- **Linear**: Modern aesthetic, confident tone, problem-solution structure
- **Stripe**: Clear explanations, respects technical audience
- **Notion**: Complete workflow demonstration, approachable
- **Slack**: Casual conversation style, quick onboarding feel

### What Makes Them Successful
- **Problem-first approach**: Start with relatable pain
- **Real scenarios**: Not generic feature tours
- **Technical respect**: Don't oversimplify for engineers
- **Clarity over flash**: Professional but not over-produced
- **Clear value**: Benefit of each feature is obvious

### What to Avoid
- **Feature dumps**: Listing capabilities without context
- **Generic stock footage**: Feels inauthentic for dev tools
- **Corporate voice**: Too formal for developer audience
- **Hype language**: "Revolutionary," "game-changing," etc.
- **No clear CTA**: Leaving viewers unsure what to do next

---

## ‚úÖ Pre-Production Checklist

Before creating the demo video, ensure:

**Content Planning:**
- [ ] Use case selected and validated with target users
- [ ] Script written and scored against rubric
- [ ] Key features to demonstrate identified (max 3-4)
- [ ] Benefits clearly articulated for each feature
- [ ] AI transparency moments scripted
- [ ] CTA and friction reducers finalized

**Production Preparation:**
- [ ] Demo environment set up with clean data
- [ ] Screen recordings planned and storyboarded
- [ ] Brand assets gathered (logo, colors, fonts)
- [ ] Voiceover artist selected (or AI voice tested)
- [ ] Timeline visualization created/ready to show
- [ ] Annotations and highlights planned

**Technical Requirements:**
- [ ] Recording software tested (1080p minimum)
- [ ] Audio equipment/setup tested
- [ ] Video editing software selected
- [ ] Export settings determined (format, resolution, compression)
- [ ] Hosting platform selected (YouTube, Vimeo, self-hosted)

**Brand Alignment:**
- [ ] Script reviewed against brand voice guidelines
- [ ] Messaging approved by marketing
- [ ] Technical accuracy verified by engineering
- [ ] Tone matches "senior engineer to colleague"
- [ ] Transparency requirements met (show AI reasoning)

---

## üéì Training: How to Use This Rubric

### For Video Creators:
1. **Script Phase**: Score your script draft against criteria before production
2. **Production**: Reference checklist to ensure all elements are captured
3. **Post-Production**: Self-evaluate rough cut and iterate before final
4. **Final Review**: Score final video and ensure >80/100 target

### For Reviewers:
1. **Independent Scoring**: Watch video and score each criterion individually
2. **Calibration**: Compare scores with other reviewers and discuss differences
3. **Prioritized Feedback**: Focus on lowest-scoring high-weight criteria first
4. **Actionable Notes**: Provide specific examples for improvements

### For Stakeholders:
1. **Quick Assessment**: Review total score and category breakdowns
2. **Focus Areas**: Identify which sections need most attention
3. **Go/No-Go**: Use 80/100 threshold for production readiness decision
4. **Iteration Planning**: Prioritize improvements based on weighted scores

---

## üìû Questions & Revisions

This rubric should evolve based on:
- **User testing**: What actually converts viewers to trial users?
- **A/B testing**: Which approaches perform best?
- **Industry trends**: What are competitors doing successfully?
- **Product evolution**: As Cronicorn matures, demo should too

**Review quarterly** and update based on:
- Video performance metrics
- User feedback from trial sign-ups
- Changes to product positioning
- New features or capabilities

---

**Version:** 1.0
**Created:** 2026-01-06
**Last Updated:** 2026-01-06
**Owner:** Marketing/Growth Team
**Status:** Ready for Use

---

## üîó Related Documents

- [Marketing Overview](./overview.md) - Overall marketing strategy and positioning
- [Brand Voice Guidelines](./brand-voice.md) - Tone, style, and messaging framework
- [Copy Cheat Sheet](./copy-cheatsheet.md) - Quick reference for messaging
- [Landing Page Blueprints](./page-blueprints.md) - Website content structure
- [SEO Strategy](./seo-strategy.md) - Video SEO and distribution

---

**Key Takeaway**: This rubric emphasizes **problem-first storytelling**, **transparent AI demonstration**, and **technical respect** for the DevOps/SRE audience. A successful Cronicorn demo video should make engineers think "this solves my exact problem" within the first 15 seconds and show them exactly how within the next 75 seconds.

---

## üé• SHOT-BY-SHOT PRODUCTION SCRIPT

**Demo Scenario:** E-Commerce Flash Sale (one example of adaptive scheduling)
**Seed Data Location:** `apps/migrator/src/seed.ts`
**Total Runtime:** 105 seconds (1:45)

**Note:** This script uses a **unified narration track** that appeals to both DevOps/SRE engineers and solo builders. The narration uses inclusive, general language while the visuals show a monitoring scenario. This approach:

- **Shows:** Monitoring use case (Track A visuals - familiar to DevOps)
- **Says:** General adaptive scheduling language (works for all audiences)
- **Signals:** Multiple use cases explicitly (monitoring, scraping, pipelines, workflows)

The result: DevOps sees a familiar scenario they trust. Solo builders hear inclusive language and see themselves mentioned. Everyone understands Cronicorn as a versatile scheduling backbone.

### Pre-Recording Setup

**Run the seed script:**
```bash
pnpm db:reset
pnpm db:migrate
pnpm tsx apps/migrator/src/seed.ts
```

**What this creates:**
- 1 job: "E-Commerce Flash Sale Demo" (all 11 endpoints)
- 18,566 runs over 7 days
- 6 AI analysis sessions with transparent reasoning
- Complete flash sale cycle: Day 6, 12:00-13:00 (25 hours ago from now)

**Dashboard navigation:**
- Open `/dashboard`
- Select job: "E-Commerce Flash Sale Demo"
- Timeline will show 7 days of data with visible spike at Day 6, 12:00-13:00

---

### SHOT 1: Hook & Problem (0-12 seconds)

**Visual:**
- Start on **dashboard homepage** showing the job dropdown
- Quick pan across timeline showing the flash sale spike (Day 6, 12:00-13:00)
- Zoom slightly into the critical phase (minutes 13-20) showing red/failed runs

**Narration:**
> "Your workload just spiked. Could be a traffic surge, a live event, maybe a pipeline going nuts. But your scheduled jobs? They're still running every 5 minutes like nothing happened."

**On-screen text:**
- "Workload: 6x increase"
- "Scheduled jobs: Still every 5 minutes"
- "Static schedules can't adapt"

**What to capture:**
- Timeline visualization clearly showing baseline (green) vs critical (red) periods
- Visible clustering of runs during flash sale window
- Success rate drop visible in timeline (baseline: mostly green, critical: lots of red)

---

### SHOT 2: Solution Introduction (12-27 seconds)

**Visual:**
- Cut to **Cronicorn logo** briefly (2 seconds)
- Transition to **dashboard overview** showing the full 7-day timeline
- Highlight the **source attribution colors**:
  - Baseline interval: Green dots (days 1-5)
  - AI interval: Blue dots (flash sale window)
  - AI one-shot: Purple dots (recovery/alert actions)
  - Clamped-min: Orange dots (critical phase)

**Narration:**
> "Meet Cronicorn. It's a job scheduler that actually adapts to what's happening. Instead of rigid schedules, it watches your workload and adjusts on the fly. Things heating up? It tightens. Things calm down? It backs off. And here's the thing‚Äîevery decision is transparent. You're always in control."

**On-screen text:**
- "Schedules adapt to what's happening"
- "Every decision explained"
- "You set the boundaries"

**What to capture:**
- Full timeline showing 7-day view (establish baseline credibility)
- Color legend visible (if UI shows it) or add annotation explaining colors
- Smooth pan from left (Day 1 baseline) to center (Day 6 flash sale) to right (Day 7 recovery)

---

### SHOT 3A: Baseline Credibility (27-35 seconds)

**Visual:**
- Zoom into **Days 1-5** on timeline
- Show **Traffic Monitor endpoint** details:
  - Name: "Traffic Monitor"
  - Baseline interval: 60,000ms (1 minute)
  - Runs: Green dots, evenly spaced
  - Success rate: 95-98%

**Narration:**
> "Let's see it in action. This is a monitoring job tracking an e-commerce site. For five days, everything's smooth. Baseline schedule, checking every minute. 98% success rate. Nice and calm."

**On-screen highlights:**
- Circle the baseline interval: "1 minute baseline"
- Arrow pointing to response body metrics: "1,000 visitors/min, 800ms page load"
- Success indicators: "98% success"

**What to capture:**
- Click on **Traffic Monitor** endpoint to show details panel
- Scroll to show **response body** with metrics:
  ```json
  {
    "traffic": 1000,
    "ordersPerMin": 40,
    "pageLoadMs": 800,
    "inventoryLagMs": 100,
    "dbQueryMs": 120
  }
  ```
- Show latest run timestamp from Days 1-5

---

### SHOT 3B: Surge Detection (35-50 seconds)

**Visual:**
- Pan timeline to **Day 6, 12:05-12:08** (Surge phase)
- Zoom into **minute 6** specifically (12:06)
- Show **AI Session #2** panel opening
- Highlight the tool call: `propose_interval(30000, 60)`

**Narration:**
> "Then things change. Activity jumps 5x. The AI catches it immediately. And here's what's cool‚Äîit shows you exactly why it's adjusting. 'Surge detected. Tightening to 30 seconds.' This same pattern works for monitoring, scraping, pipelines... basically anything that needs to adapt."

**AI Reasoning visible on screen:**
> "Traffic surge detected: 1,000 ‚Üí 5,100 requests/min. Tightening to 30 seconds to catch issues faster."

**On-screen highlights:**
- Circle the **AI Session** entry at **12:06**
- Highlight the tool call: `propose_interval(30000, 60)`
- Arrow showing: "1 minute ‚Üí 30 seconds (2x faster)"
- Show response body changing: "traffic: 5100, pageLoadMs: 1850"

**What to capture:**
- Click on **AI Session** from timeline (if clickable) or navigate to AI sessions panel
- Show full reasoning text
- Show **tool calls** section with `get_latest_response`, `get_response_history`, `propose_interval`
- Token usage: 1,250 tokens
- Duration: 420ms

---

### SHOT 3C: Critical Response & Multi-Tier Actions (50-68 seconds)

**Visual:**
- Pan timeline to **Day 6, 12:13-12:20** (Critical phase)
- Show timeline changing colors: Blue (AI interval) ‚Üí Orange (clamped-min) ‚Üí Purple (one-shot)
- Split screen or quick cuts between:
  1. **Traffic Monitor** - interval clamped to 20 seconds (min constraint)
  2. **Slow Page Analyzer** - activated (was paused, now running)
  3. **Cache Warmup** - purple one-shot execution
  4. **Emergency Oncall Page** - purple one-shot execution

**Narration:**
> "Now things are getting critical. Watch how it responds. The AI maxes out at your minimum constraint. It wakes up related jobs that were paused. Triggers one-shot actions. Escalates when it needs to. All automatic. All visible."

**On-screen highlights:**
- **Traffic Monitor**: "Interval: 20s (min constraint enforced)"
- **Slow Page Analyzer**: "Status: ACTIVATED (was paused)"
- **Cache Warmup**: "Triggered: One-shot recovery action"
- **Emergency Oncall**: "Paged oncall: Critical threshold"

**What to capture:**
- Show **AI Session #3** at **12:15**:
  - Tool call: `pause_until(null)` - unpausing Slow Page Analyzer
  - Tool call: `propose_next_time(5000, 5)` - cache warmup in 5 seconds
  - Reasoning: "CRITICAL: Page load times at 4600ms..."
- Show endpoint list with **4 tiers** visible:
  - Health (3): Continuous, tightened
  - Investigation (2): Now active (was paused)
  - Recovery (2): One-shot actions (purple dots)
  - Alert (4): Escalation (purple dots)
- Response body metrics at critical peak:
  ```json
  {
    "traffic": 6000,
    "pageLoadMs": 4500,
    "ordersPerMin": 120,
    "inventoryLagMs": 600,
    "dbQueryMs": 1200
  }
  ```

---

### SHOT 3D: Recovery & Automatic Reversion (68-85 seconds)

**Visual:**
- Pan timeline to **Day 6, 12:21-12:39** (Recovery phase)
- Show colors shifting back: Orange ‚Üí Blue ‚Üí Green
- Show metrics improving in response bodies
- Jump to **Day 6, 13:05** - hint expiration moment

**Narration:**
> "Things start recovering. The AI sees it and eases back to baseline. Then 60 minutes later, all the AI adjustments expire automatically. Your baseline schedule takes over again. No manual tweaking. No runaway automation. It just... adapts and gets out of your way."

**On-screen highlights:**
- **Minute 28** (12:28): "AI Session #4: Recovery confirmed"
- Show tool call: `propose_interval(60000, 30)` - back to 1 minute
- **Minute 65** (13:05): "AI Session #5: Hints expired"
- Show reasoning: "All AI hints expired (60-min TTL). Baseline schedules fully resumed."
- **Day 7, 08:00**: "AI Session #6: Stability confirmed"

**What to capture:**
- Show **AI Session #4** reasoning: "Recovery confirmed. Traffic declining to 1,400/min, page load improved to 1,050ms..."
- Show timeline return to green (baseline-interval source)
- Show **AI Session #5** with TTL expiration message
- Pan to Day 7 showing normal baseline pattern restored

**Metrics to show:**
- Recovery phase: traffic: 1500, pageLoadMs: 1100, ordersPerMin: 50
- Post-expiration: traffic: 1050, pageLoadMs: 820, ordersPerMin: 42

---

### SHOT 4: Key Differentiators (85-95 seconds)

**Visual:**
- Cut to **endpoint configuration view** showing:
  - **Traffic Monitor** endpoint details
  - Min interval: 20,000ms (20 seconds)
  - Max interval: 300,000ms (5 minutes)
  - Baseline interval: 60,000ms (1 minute)
- Show **AI Sessions** list panel with all 6 sessions visible
- Quick highlight of **source attribution** legend
- **[OPTIONAL]** Show use cases callout (see next section)

**Narration:**
> "So what makes this different? First, it's completely transparent. You saw exactly why every schedule changed. Second, you set the boundaries. The AI stays within your min and max constraints. And third, it works fine without AI. Your baseline schedules always run no matter what."

**On-screen text:**
- "‚úì Every decision explained"
- "‚úì Min/max constraints enforced"
- "‚úì Works without AI"

**[OPTIONAL] Use Cases Diversity Callout:**

If you want to explicitly signal versatility, add this 2-3 second visual callout during this shot:

**On-screen text (animated bullet points):**
```
Use Cronicorn for:
‚Ä¢ System monitoring & health checks
‚Ä¢ Live data scraping & API fetching
‚Ä¢ ETL pipelines & data syncing
‚Ä¢ Webhook processing & event handling
‚Ä¢ CI/CD workflows & deployments
```

**Note:** This is optional but recommended if you want to maximize appeal to both DevOps and solo builder audiences. The callout takes 2-3 seconds and appears while the narration plays.

**What to capture:**
- Show **min/max constraint fields** in endpoint configuration
- Show **all 6 AI sessions** in list view with timestamps
- Show **baseline interval** field highlighted
- Optional: Show AI toggle or configuration (if exists in UI)

---

### SHOT 5: Call-to-Action (95-105 seconds)

**Visual:**
- Cut back to **dashboard overview** with full 7-day timeline
- Zoom out slightly to show complete story: baseline ‚Üí surge ‚Üí recovery
- Fade to **Cronicorn logo** on clean background
- **CTA overlay** appears

**Narration:**
> "Stop fighting rigid schedules. Whether you're monitoring systems, scraping live data, running pipelines, or orchestrating workflows‚ÄîCronicorn just adapts. Time to schedule smarter."

**On-screen text (large, clear):**
```
Get Early Access
cronicorn.com
No credit card required
```

**Secondary text (smaller, bottom):**
```
14-day free trial ‚Ä¢ Set up in 5 minutes
```

**What to capture:**
- Final pan of complete timeline showing the full adaptation cycle
- Clean fade to logo and CTA
- CTA visible for 5-8 seconds before fade out

---

### POST-PRODUCTION CHECKLIST

**Annotations to Add:**
- [ ] **Timeline colors legend**: Add persistent legend explaining source types
  - Green = Baseline interval
  - Blue = AI-adjusted interval
  - Purple = AI one-shot action
  - Orange = Clamped to min/max
- [ ] **Metrics callouts**: Add animated boxes around key metrics when mentioned
- [ ] **AI reasoning highlights**: Box or highlight AI session reasoning text for readability
- [ ] **Timeline scrubber**: Optional: Add timestamp indicator showing current moment being discussed
- [ ] **Endpoint tier badges**: Add visual indicators for Health/Investigation/Recovery/Alert tiers

**Transitions:**
- Smooth pan/zoom on timeline (no cuts within timeline shots)
- Quick fade transitions between major sections (1-2 frames)
- No flashy effects‚Äîkeep professional/developer tool aesthetic

**Audio:**
- Narration: "Senior engineer explaining to colleague" tone
- Background music: Optional subtle ambient (if used, keep very low volume)
- Sound effects: Minimal‚Äîonly if needed for emphasis (e.g., alert sound on oncall page)

**Text overlays:**
- Font: Sans-serif (Inter, SF Pro, or similar)
- Size: Large enough to read at 1080p on laptop (minimum 24px equivalent)
- Colors: Brand blue (#3B82F6) for highlights, white for text
- Duration: 3-5 seconds per text overlay (long enough to read twice)

**Branding:**
- Logo appears: Start (2 seconds) and End (5-8 seconds)
- Brand colors: Use blue accent consistently
- No heavy branding during demo (keep focus on product)

---

### SEED DATA REFERENCE (Quick Lookup)

**Important:** These are the specific metrics used in the demo scenario (e-commerce flash sale). The **narration** should reference these visually but **speak to the general concept** of adaptive scheduling that applies to any use case.

**Timeline Structure:**
- **Days 1-5**: Baseline establishment (5 days before workload change)
- **Day 6, 12:00-13:00**: Workload surge event (60 minutes)
- **Day 7**: Post-event stabilization

**Flash Sale Phases (Day 6, 12:00-13:00):**
| Phase | Minutes | Traffic | Success | Page Load | AI Action |
|-------|---------|---------|---------|-----------|-----------|
| Baseline | 0-4 | 1,000/min | 98% | 800ms | None |
| Surge | 5-8 | 5,000/min | 92% | 1,800ms | ‚Üí 30s intervals |
| Strain | 9-12 | 5,500/min | 85% | 3,200ms | Activate diagnostics |
| Critical | 13-20 | 6,000/min | 60% | 4,500ms | Cache + Oncall |
| Recovery | 21-39 | 1,500/min | 95% | 1,100ms | ‚Üí Baseline |
| Post-Sale | 40+ | 1,050/min | 98% | 820ms | Hints expire |

**AI Sessions (6 total):**
1. **11:45** (Pre-sale): "Normal patterns, no action needed"
2. **12:06** (Surge): `propose_interval(30000)` - "Tightening to 30s"
3. **12:15** (Critical): `pause_until(null)`, `propose_next_time` - "Emergency recovery"
4. **12:28** (Recovery): `propose_interval(60000)` - "Back to baseline"
5. **13:05** (Expiration): "All hints expired (60-min TTL)"
6. **Day 7, 08:00** (Stability): "Post-event stability confirmed"

**Endpoints to Feature:**
- **Traffic Monitor** (Health): 10,112 runs, 1min baseline ‚Üí 20s min
- **Slow Page Analyzer** (Investigation): 8 runs, activated during strain
- **Cache Warmup** (Recovery): 2 runs, one-shot actions with 10min cooldown
- **Emergency Oncall Page** (Alert): 1 run, 2-hour cooldown

**Run Counts (Total: 18,566):**
- Health tier: 18,541 runs (continuous monitoring)
- Investigation: 14 runs (flash sale only)
- Recovery: 4 runs (one-shot with cooldowns)
- Alert: 7 runs (one-shot with cooldowns)

**Source Distribution (During Flash Sale):**
- Baseline: 15-40% (depending on phase)
- AI Interval: 60-85% (majority during active adaptation)
- AI One-shot: 5-10% (recovery/alert actions)
- Clamped-min: ~5% (critical phase)

---

### RECORDING TIPS

**Screen Recording Settings:**
- Resolution: 1920x1080 (1080p minimum)
- Frame rate: 30fps (60fps if smooth panning is critical)
- Software: QuickTime (macOS), OBS Studio, or similar
- Cursor: Smooth, deliberate movements (not erratic)
- Hide unnecessary UI: Browser bookmarks bar, dock (if possible)

**Dashboard Prep:**
- Clear any notification badges or distractions
- Ensure browser is full-screen or zoomed appropriately
- Have all panels/views pre-navigated (practice run first)
- Consider multiple takes per shot (choose best later)

**Timing the Recording:**
- Seed data is relative to "NOW" so timing is consistent
- Flash sale always appears at Day 6, 12:00-13:00 (25 hours ago)
- Run seed script fresh before recording to ensure clean data

**Multiple Takes Strategy:**
- Record each SHOT separately (easier to edit)
- Record 2-3 takes of each shot
- Keep recording even if you make small mistakes (fix in post)
- Record timeline pans slowly (speed up in post if needed)

---

---

## üéØ UNIFIED NARRATION: APPEALING TO BOTH AUDIENCES

**Why This Matters:** The unified narration approach appeals to both DevOps/SRE engineers and solo builders simultaneously. Instead of choosing between audiences, we use inclusive language that lets each viewer see themselves in the product.

**The Strategy:**
- **Visuals:** Show familiar DevOps monitoring scenario (builds credibility with technical audience)
- **Language:** Use general terms that apply to all scheduling use cases
- **Explicit Mentions:** Name multiple use cases (monitoring, scraping, pipelines, workflows) so no one feels excluded

### Original Track B Context (For Reference)

Indie developers and solo app builders represent a massive untapped audience. They face the same adaptive scheduling challenges but frame them differently:

### Real-World Solo Builder Scenarios

**1. Fantasy Sports App (Main Example)**
- **Stats Scraper**: Checks frequently during live games (every 20s), backs off when no games (hourly)
- **Player Images**: Fetches from Google Images API with 100 calls/hour limit‚ÄîAI throttles to stay under quota
- **Ratings Calculator**: Runs only after new game data detected (not on fixed schedule)
- **Injury Updates**: Checks during games, pauses during off-season

**2. Content Aggregator / Newsletter App**
- **RSS Feed Scraper**: Checks popular blogs every 5 minutes, inactive blogs every 6 hours
- **Image Optimizer**: Processes new images immediately, skips when no new content
- **Email Sender**: Batches sends to respect SendGrid rate limits (10k/hour)
- **Dead Link Checker**: Runs weekly unless engagement drops (then increases to catch issues)

**3. Price Tracking / Deal Finder**
- **Product Scraper**: Checks frequently during sales seasons, weekly during quiet periods
- **Price Drop Detector**: Increases frequency when Black Friday approaches
- **Stock Checker**: Pings APIs every minute for hot items, hourly for others
- **Deal Notifier**: Triggers webhook only when thresholds hit (not on schedule)

**4. Social Media Dashboard**
- **Twitter API**: Respects 500 calls/15min limit‚ÄîAI automatically throttles
- **Instagram Scraper**: Backs off when rate limited, resumes when window resets
- **Trending Topics**: Checks every 2 minutes during peak hours, hourly at night
- **Sentiment Analyzer**: Processes only when new posts detected (not fixed schedule)

**5. Data Pipeline / ETL App**
- **Source API**: Fetches new records frequently during business hours, backs off at night
- **Transform Job**: Runs only when input data changes (not every 5 minutes)
- **Database Sync**: Increases frequency when backlog grows, throttles when caught up
- **Export Job**: Batches efficiently to avoid overwhelming downstream systems

### How the Demo Translates

| Demo Visual | DevOps Interpretation | Solo Builder Interpretation |
|-------------|----------------------|----------------------------|
| "Traffic surge" | System under load | Live games started / hot product dropped |
| "Tighten to 30s" | Catch incidents faster | Capture live updates / maximize freshness |
| "Activate diagnostics" | Debug performance | Fetch related data (player images, stock levels) |
| "Cache warmup" | Performance optimization | Pre-fetch related data efficiently |
| "Oncall page" | Human escalation | Trigger user notification webhook |
| "Recovery" | Load decreased | Event finished / activity normalized |
| "Hints expire" | Return to baseline | Go back to hourly/daily checks |

### Key Messaging Differences

**Track A (DevOps) Emphasizes:**
- Reliability, uptime, incident response
- Reducing alert fatigue, firefighting
- Team efficiency, oncall burden
- Enterprise concerns: observability, SLAs

**Track B (Solo Builders) Emphasizes:**
- API quota management, cost savings
- Time-sensitive data capture (games, sales, trends)
- Simplicity (no DevOps team needed)
- Indie/startup concerns: bootstrapping, scrappy solutions

### Why the Unified Approach Works

**Same product, universal language:**
- Everyone: "Stop fighting rigid schedules"
- Inclusive: "Schedules adapt to your reality"

**Same solution, broad applicability:**
- Universal: "Adapts to what's happening"
- Inclusive: Works for monitoring, scraping, pipelines, workflows

**Same transparency, universal trust:**
- Everyone: "Every decision explained"
- Universal: "You set the boundaries"

**Key Advantage:** No viewer feels excluded. DevOps engineers see a familiar scenario. Solo builders hear inclusive language and see their use cases mentioned explicitly.

### Production Recommendation

**Primary Strategy (Recommended):** Use the **Unified Narration** approach documented above:
- Shows monitoring scenario (builds technical credibility)
- Uses inclusive language throughout (welcomes all audiences)
- Explicitly lists multiple use cases (no one feels excluded)
- Single video appeals to both DevOps and solo builders

**Benefits:**
- ‚úÖ No need to maintain two separate videos
- ‚úÖ Works across all marketing channels (homepage, social, docs)
- ‚úÖ DevOps viewers trust the technical scenario
- ‚úÖ Solo builders see themselves in the language and use case mentions
- ‚úÖ Positions Cronicorn as versatile scheduling backbone (not niche tool)

**Alternative: Audience-Specific Videos (Optional)**

If you want to create targeted variations for specific channels:

**DevOps-Focused Version:**
- Use unified narration but emphasize monitoring terminology in CTA
- Target: Conference talks, DevOps blogs, enterprise outreach

**Solo Builder-Focused Version:**
- Use unified narration but emphasize scraping/API terminology in CTA
- Target: Indie Hacker communities, r/SideProject, Product Hunt

**Note:** The unified approach is recommended for most use cases. Only create audience-specific versions if you have distribution channels that clearly segment these audiences.

### Key Language Patterns (Quick Reference)

**‚úÖ DO: Sound conversational and natural**
- Use contractions: "it's", "you're", "don't", "can't"
- Short, punchy sentences
- Natural pauses and flow
- Talk like you're explaining to a friend
- "Things heating up? It tightens." not "When conditions warrant, it adjusts parameters"

**‚úÖ DO: Use inclusive, general terms**
- "Your workload" (not "your system" or "your API")
- "Scheduled jobs" (not "health checks" or "scrapers")
- "Things change" / "Activity jumps" (not "traffic surges" or "live events")
- "It just adapts" (not "adapts to load" or "adapts to data")
- "Things heating up" / "getting critical" (not "during incidents" or "during games")
- "Related jobs" / "wakes up jobs" (not "diagnostics" or "scrapers")
- "Things recover" / "eases back" (not "system stabilizes" or "games finish")

**‚ùå DON'T: Sound robotic or overly formal**
- Avoid: Formal structures like "One‚Äîfirst feature. Two‚Äîsecond feature."
- Avoid: Perfect grammar at the expense of natural flow
- Avoid: Corporate buzzwords or marketing speak

**‚ùå DON'T: Use domain-specific language**
- Avoid: "health checks," "observability" (too DevOps-specific)
- Avoid: "API quota," "rate limits" (too solo-builder-specific)
- Avoid: "production," "incidents," "oncall" (enterprise-focused)

**üéØ DO: Explicitly list use cases**
- Mention multiple use cases: "monitoring, scraping, pipelines, workflows"
- Use casual transitions: "basically anything that needs to adapt"
- In CTA, list 3-4 concrete examples

This ensures the narration feels natural while appealing to both DevOps engineers and solo builders.

---

**Script Version:** 1.3 (Unified Narration - Natural, Conversational Language)
**Based on Seed Data:** `apps/migrator/src/seed.ts`
**Ready for:** Production recording (single unified track)
**Estimated Edit Time:** 3-5 hours (single video with broad appeal)
